{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyrm3uesjcTJ"
      },
      "source": [
        "# Stellar Classification\n",
        "\n",
        "In this notebook, we investigate the Gaia dataset in order to explore the stellar classification via neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRt4E_GqU3KD"
      },
      "outputs": [],
      "source": [
        "# Libraries for plotting and manipulating data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Libraries for studying the graphs\n",
        "import networkx as nx\n",
        "import community as community_louvain\n",
        "import community.community_louvain as community_louvain\n",
        "from scipy.spatial import KDTree\n",
        "\n",
        "\n",
        "#Libraries for implenting and training a neural network\n",
        "from keras.layers import Dense, Input, Dropout, BatchNormalization\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Library for the creation of the tables\n",
        "from tabulate import tabulate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_ANFuQ2t6lY"
      },
      "source": [
        "# Set Global Configuration Options\n",
        "\n",
        "It is nice to specify global configuration options at the start of the notebook, so tweaking certain parameters does not require much \"scrolling\" down the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxPJI46rt9S6"
      },
      "outputs": [],
      "source": [
        "# set a seed\n",
        "seed=42\n",
        "\n",
        "def space():\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl61qLhtTnrr"
      },
      "source": [
        "# Uploading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyHwNDtGNuSj"
      },
      "outputs": [],
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()\n",
        "\n",
        "data = pd.read_csv('Data_Gaia_reduced.csv',nrows=32000) #132000\n",
        "\n",
        "# Show the shapes of the dataset\n",
        "data.shape\n",
        "\n",
        "# Get column names\n",
        "columns_list = data.columns.tolist()  # Extract column names and convert to a list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23073f5f-915d-4c4d-b5b8-7d2568f485ff"
      },
      "source": [
        "# Dataset Preparation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Before doing any kind of machine learning, it’s important to understand the nature of the data with one is dealing with. We check some information about the dataset, including how many missing values there are, the shape of the data, as well as the dtypes of each of the columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rOMjE8WTxbs"
      },
      "source": [
        "## Structure of the data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idWod7kpMNsq"
      },
      "source": [
        "As always with new data, it is essential that we understand what the different columns means. Luckily, the dataset brief gives us some insight about what they mean:\n",
        "\n",
        "- RA_ICRS: Right ascension in the ICRS (International Celestial Reference System) coordinate system.\n",
        "- DE_ICRS: Declination in the ICRS coordinate system.\n",
        "- Teff: Estimated effective temperature of the celestial object by Gaia in Kelvins.\n",
        "- Dist: Distance to the celestial object: inverse of the parallax, in parsecs.\n",
        "- Rad: Object radius estimate in terms of solar radius.\n",
        "- Rad-Flame: Object radius estimate in terms of solar radius.\n",
        "- Lum: Estimated object luminosity in terms of solar luminosity.\n",
        "- Mass: Mass estimate in terms of solar mass.\n",
        "- Age: Celestial object age in giga years.\n",
        "- z: Redshift in km/s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01YGJgZBT3Ot"
      },
      "outputs": [],
      "source": [
        "# General informations on the dataset\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICXoLMPiMIR0"
      },
      "source": [
        "### Removing Trailing Whitespace for Stellar Classes\n",
        "It appears that the stellar classes have a lot of trailing whitespaces, which might impact our readability. Let's remove these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "io6rfTy4tMEc"
      },
      "outputs": [],
      "source": [
        "# Remove null rows\n",
        "data.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvS6sn8GOlxW"
      },
      "outputs": [],
      "source": [
        "# General informations on the dataset\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ6VMpUJNWxa"
      },
      "source": [
        "### Simple description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0iPHus_T-nH"
      },
      "outputs": [],
      "source": [
        "# Sum up of statistics of the features\n",
        "data.describe() #.to_latex()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1279eSBtUfC"
      },
      "outputs": [],
      "source": [
        "# Presents the first non-null rows\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9LwHBCapTzS"
      },
      "source": [
        "# Data analysis\n",
        "\n",
        "We explore the statistics and the behaviors of the data under consideration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyMtD0rFxyR4"
      },
      "source": [
        "### Spatial distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DoE7R1Qx8BN"
      },
      "outputs": [],
      "source": [
        "# 3D Scatter Plot\n",
        "def plot_3d_distribution(data):\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter(data['RA_ICRS'], data['DE_ICRS'], data['Dist'], c=data['Dist'], cmap='viridis', alpha=0.7)\n",
        "    ax.set_title(\"3D Spatial Distribution of Stars\")\n",
        "    ax.set_xlabel(\"Right Ascension (RA)\")\n",
        "    ax.set_ylabel(\"Declination (Dec)\")\n",
        "    ax.set_zlabel(\"Distance (parsecs)\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 2D Scatter Plots\n",
        "def plot_2d_projections(data):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # RA vs Dec\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(data['RA_ICRS'], data['DE_ICRS'], c=data['Dist'], cmap='viridis', alpha=0.7)\n",
        "    plt.colorbar(label='Distance (parsecs)')\n",
        "    plt.title(\"RA vs Dec\")\n",
        "    plt.xlabel(\"Right Ascension (RA)\")\n",
        "    plt.ylabel(\"Declination (Dec)\")\n",
        "\n",
        "    # RA vs Distance\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.scatter(data['RA_ICRS'], data['Dist'], c=data['DE_ICRS'], cmap='viridis', alpha=0.7)\n",
        "    plt.colorbar(label='Declination (Dec)')\n",
        "    plt.title(\"RA vs Distance\")\n",
        "    plt.xlabel(\"Right Ascension (RA)\")\n",
        "    plt.ylabel(\"Distance (parsecs)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot the distributions\n",
        "plot_3d_distribution(data)\n",
        "space()\n",
        "plot_2d_projections(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnmKzV8ly4Se"
      },
      "outputs": [],
      "source": [
        "# Check for clustering in Distance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(data['Dist'], bins=30, color='skyblue', edgecolor='black', alpha=0.7)\n",
        "plt.title(\"Histogram of Distances\")\n",
        "plt.xlabel(\"Distance (parsecs)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "\n",
        "# Normalize RA, Dec, and Distance to the range [0, 1] for analysis\n",
        "scaler = MinMaxScaler()\n",
        "data_normalized = data[['RA_ICRS', 'DE_ICRS', 'Dist']].copy()\n",
        "data_normalized[['RA_ICRS', 'DE_ICRS', 'Dist']] = scaler.fit_transform(data_normalized)\n",
        "\n",
        "# Subsampling\n",
        "# If clustering is evident in the Distance histogram, subsample the data\n",
        "def subsample_data(data, distance_column, bins=5, sample_size=100):\n",
        "    data['DistanceBin'] = pd.cut(data[distance_column], bins=bins)\n",
        "    sampled_data = data.groupby('DistanceBin').apply(lambda x: x.sample(n=min(len(x), sample_size), random_state=42))\n",
        "    return sampled_data.reset_index(drop=True)\n",
        "\n",
        "# Subsampled data based on Distance\n",
        "data_subsampled = subsample_data(data, 'Dist', bins=5, sample_size=100)\n",
        "\n",
        "# Plot the 3D distribution of the subsampled data\n",
        "plot_3d_distribution(data_subsampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRJcofSbvU7C"
      },
      "source": [
        "### Generation of the histograms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJA84srYGj66",
        "outputId": "57807cb8-e02b-4753-b30d-053cafbc2cd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting diptest\n",
            "  Downloading diptest-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from diptest) (5.9.5)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.11/dist-packages (from diptest) (1.26.4)\n",
            "Downloading diptest-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (197 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/197.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m197.5/197.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diptest\n",
            "Successfully installed diptest-0.8.2\n"
          ]
        }
      ],
      "source": [
        "!pip install diptest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wH58QaB5UzhO"
      },
      "outputs": [],
      "source": [
        "#Useful library for studying the system\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import gaussian_kde\n",
        "from diptest import diptest  # Install using !pip install diptest\n",
        "from scipy.stats import kurtosis, skew\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9QDEjMKPZp2"
      },
      "outputs": [],
      "source": [
        "# It creates a histogram with a (optional) Gaussian on it\n",
        "def hist_with_gaussian(df, column, bins, color_graph, Gaussian, subject):\n",
        "    # Generation of the histogram:\n",
        "    count, bins, ignored = plt.hist(df[column], bins, density=True, alpha=0.6, color=color_graph, label=f'Histogram of the {subject}')\n",
        "\n",
        "    # Evaluation of the mean (mu) and standard deviation (std) for the data\n",
        "    mu, std = norm.fit(df[column])\n",
        "\n",
        "    xmin, xmax = plt.xlim()  # To get fixed x-axis limits for the distribution\n",
        "    x = np.linspace(xmin, xmax, 15000)\n",
        "    # Gaussian distribution\n",
        "    p = norm.pdf(x, mu, std)\n",
        "\n",
        "    # Plot\n",
        "    if Gaussian:\n",
        "        sums, bins = np.histogram(df[column].values, bins=bins, density=True, range=[xmin, xmax])\n",
        "        bin_centers = (bins[:-1] + bins[1:]) / 2\n",
        "        expected = (1 / np.sqrt(2 * np.pi * std ** 2)) * np.exp(-(bin_centers - mu) ** 2 / (2 * std ** 2))\n",
        "\n",
        "        plt.plot(bin_centers, expected, 'o', color='blue', label='Expected bins center')\n",
        "        plt.plot(bin_centers, sums, 'o', color='green', label='Predicted bins center')\n",
        "        plt.plot(x, p, 'k', linewidth=2, color='#E76F51', label=\"Gaussian\")\n",
        "        plt.title(f\"Histogram of the {subject}; result: {round(mu, 3)} ± {round(std, 3)}\")\n",
        "        plt.ylabel(\"Frequency\")\n",
        "        plt.xlabel(column)\n",
        "\n",
        "    # Compute kurtosis and skewness\n",
        "    data_kurtosis = kurtosis(df[column].dropna(), fisher=True)  # Fisher=True for excess kurtosis\n",
        "    data_skewness = skew(df[column].dropna())\n",
        "\n",
        "    # Return the computed mean, std, kurtosis, and skewness\n",
        "    return mu, std, data_kurtosis, data_skewness\n",
        "\n",
        "# It prints the data of the Gaussian\n",
        "def print_values(data_print):\n",
        "    print('\\n')\n",
        "    print('#### SUMMARY OF THE DATA: ####')\n",
        "    headers = ['Subject', 'Mean value', 'Std Dev', 'Kurtosis', 'Skewness']  # Adding headers for kurtosis and skewness\n",
        "    print(tabulate(data_print, headers=headers, tablefmt='latex', colalign=(\"center\", \"center\", \"center\", \"center\", \"center\")))\n",
        "    print('############################################')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM2El1A0znqT"
      },
      "outputs": [],
      "source": [
        " ####################     Statistical Measures #########################\n",
        "''' Test for bimodality using methods like the Hartigan's Dip Test or visual inspection of KDE plots.'''\n",
        "\n",
        "# Visual Inspection: KDE Plots\n",
        "def plot_kde_with_histogram(data, column):\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    # Plot KDE and histogram\n",
        "    sns.kdeplot(data[column], color='#2A9D8F', label='KDE')\n",
        "    sns.histplot(data[column], bins=30, color='#F4A261', edgecolor=None, label=f'Histogram of the {column}', stat=\"density\")\n",
        "\n",
        "\n",
        "    # Fit a normal distribution to the data\n",
        "    mu, std = norm.fit(data[column])\n",
        "\n",
        "    # Generate x-values for the Gaussian curve\n",
        "    xmin, xmax = plt.xlim()\n",
        "    x = np.linspace(xmin, xmax, 100)\n",
        "\n",
        "    # Calculate the Gaussian curve\n",
        "    p = norm.pdf(x, mu, std)\n",
        "\n",
        "    '''\n",
        "    sums, bins = np.histogram(data[column].values, bins=30, density=True, range=[xmin, xmax])\n",
        "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
        "    expected = (1 / np.sqrt(2 * np.pi * std ** 2)) * np.exp(-(bin_centers - mu) ** 2 / (2 * std ** 2))\n",
        "\n",
        "    plt.plot(bin_centers, expected, 'o', color='blue', label='Expected bins center')\n",
        "    plt.plot(bin_centers, sums, 'o', color='green', label='Predicted bins center')\n",
        "    '''\n",
        "\n",
        "    # Plot the Gaussian curve\n",
        "    plt.plot(x, p, 'k', linewidth=2, color='#E76F51', label='Gaussian')\n",
        "    plt.vlines(mu, 0, plt.ylim()[1], label=\"mean\", color='#264653', ls='--', lw=1.4)\n",
        "\n",
        "\n",
        "    plt.title(f'KDE, histogram and gaussian for {column}')\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Density')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Hartigan's Dip Test for Bimodality\n",
        "def test_bimodality(data, column):\n",
        "    values = data[column].dropna().values\n",
        "    dip_stat, p_value = diptest(values)\n",
        "    return dip_stat, p_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Z9O92TICcsq"
      },
      "outputs": [],
      "source": [
        "#### Implementation of the function and plot ####\n",
        "\n",
        "# Implementation of the function and plot\n",
        "data_print = []  # Initialize an empty list to store data\n",
        "for column in columns_list:\n",
        "    # Evaluation of the mean, std, kurtosis, and skewness\n",
        "    mu, std, kurt, skewness = hist_with_gaussian(data, column, bins=20, color_graph='#F4A261', Gaussian=True, subject=column)\n",
        "\n",
        "    # Plot annotation\n",
        "    plt.vlines(mu, 0, plt.ylim()[1], label=\"mean\", color='#264653', ls='--', lw=1.4)\n",
        "\n",
        "    # Full plot configuration and show\n",
        "    plt.title(f\"Histogram of {column}\")\n",
        "    plt.tight_layout()\n",
        "    plt.legend(loc=\"best\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    data_print.append([column, mu, std, kurt, skewness])  # Append data to the list\n",
        "\n",
        "    print('##############################################################')\n",
        "    print('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-_mvf6T7CKu"
      },
      "outputs": [],
      "source": [
        "# Show the relevant characteristics of the histograms\n",
        "print_values(data_print)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7J3atC4z2qn"
      },
      "outputs": [],
      "source": [
        "# Example: Visualize all numeric columns\n",
        "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
        "for column in numeric_columns:\n",
        "    plot_kde_with_histogram(data, column)\n",
        "\n",
        "\n",
        "# Test for bimodality in all numeric columns\n",
        "print(\"\\nHartigan's Dip Test Results:\")\n",
        "for column in numeric_columns:\n",
        "    dip_stat, p_value = test_bimodality(data, column)\n",
        "    print(f\"Column: {column} | Dip Statistic: {dip_stat:.4f} | p-value: {p_value:.4e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4URswcd8sffV"
      },
      "outputs": [],
      "source": [
        "# Plot original and log-transformed Luminosity\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "sns.histplot(data['Lum'], bins=50, kde=True, ax=axes[0])\n",
        "axes[0].set_title(\"Original Luminosity Distribution\")\n",
        "\n",
        "sns.histplot(data['Lum_log'], bins=50, kde=True, ax=axes[1])\n",
        "axes[1].set_title(\"Log-Transformed Luminosity Distribution\")\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BM6rIpTbjy88"
      },
      "outputs": [],
      "source": [
        "# Create the histogram using seaborn's histplot: Mass vs Age\n",
        "sns.histplot(data=data, x='Mass', y='Age', bins=30, cbar=True)\n",
        "\n",
        "plt.title(\"Distribution of 'Age' with Respect to 'Mass'\")\n",
        "plt.xlabel('Mass [Solar Mass]')\n",
        "plt.ylabel('Age [Gyr]')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRvMPVFwxykr"
      },
      "outputs": [],
      "source": [
        "# Create the histogram using seaborn's histplot: Luminosity vs Age\n",
        "sns.histplot(data=data, x='Lum', y='Age', bins=30, cbar=True)\n",
        "\n",
        "plt.title(\"Distribution of 'Age' with Respect to 'Lum'\")\n",
        "plt.ylabel('Age [Gyr]')\n",
        "plt.xlabel('Luminosity (Lum) [Solar Units]')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7wUBV-WyxGt"
      },
      "outputs": [],
      "source": [
        "# Create the histogram using seaborn's histplot: Redshift vs Age\n",
        "sns.histplot(data=data, x='z', y='Age', bins=30, cbar=True)\n",
        "\n",
        "plt.title(\"Distribution of 'Age' with Respect to 'z'\")\n",
        "plt.ylabel('Age [Gyr]')\n",
        "plt.xlabel('Redshift (z) [km/s]')\n",
        "\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UrY_2pMgv_TG"
      },
      "outputs": [],
      "source": [
        "# Create the histogram using seaborn's histplot: Effective Temperature vs Age\n",
        "sns.histplot(data=data, x='Teff', y='Age', bins=30, cbar=True)\n",
        "\n",
        "plt.title(\"Distribution of 'Age' with Respect to 'Teff'\")\n",
        "plt.xlabel('Effective Temperature (Teff) [K]')\n",
        "plt.ylabel('Age [Gyr]')\n",
        "\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "omEsZ5C2i3JQ"
      },
      "outputs": [],
      "source": [
        "# Create the histogram using seaborn's histplot: Radius vs Age\n",
        "sns.histplot(data=data, x='Rad-Flame', y='Age', bins=30, cbar=True)\n",
        "\n",
        "plt.title(\"Distribution of 'Age' with Respect to 'Rad-Flame'\")\n",
        "plt.xlabel('Radius [Solar Radius]')\n",
        "plt.ylabel('Age [Gyr]')\n",
        "\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0iuMjk_jDh4"
      },
      "outputs": [],
      "source": [
        "# Create the histogram using seaborn's histplot: log(g) vs Age\n",
        "sns.histplot(data=data, x='logg', y='Age', bins=30, cbar=True)\n",
        "\n",
        "plt.title(\"Distribution of 'Age' with Respect to 'log(g)'\")\n",
        "plt.xlabel('log(g)')\n",
        "plt.ylabel('Age [Gyr]')\n",
        "\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L4PnaV6jL7I"
      },
      "outputs": [],
      "source": [
        "# Create the histogram using seaborn's histplot: [Fe/H] vs Age\n",
        "sns.histplot(data=data, x='[Fe/H]', y='Age', bins=30, cbar=True)\n",
        "\n",
        "plt.title(\"Distribution of 'Age' with Respect to '[Fe/H]'\")\n",
        "plt.xlabel('[Fe/H]')\n",
        "plt.ylabel('Age [Gyr]')\n",
        "plt.grid(True, linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIZgUUl0Rk2o"
      },
      "source": [
        "### Correlation Matrix\n",
        "\n",
        "We can investigate the correlation matrix. A correlation matrix is a statistical technique used to evaluate the relationship between two variables in a data set. The matrix is a table in which every cell contains a correlation coefficient, where 1 is considered a strong relationship between variables, 0 a neutral relationship and -1 a not strong relationship."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSjpcd-wa9Ts"
      },
      "outputs": [],
      "source": [
        "#creation of the correlation matrix\n",
        "data.corr(method='pearson')\n",
        "\n",
        "#Plotting of the figure\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.title(\"Correlation Matrix of the Dataset\")\n",
        "sns.heatmap(data.corr(method='pearson'), annot=True, cmap='coolwarm')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaogibV_zO6f"
      },
      "source": [
        "### Box plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKfSX7KuzQgm"
      },
      "outputs": [],
      "source": [
        "############## box plot ##################\n",
        "\n",
        "# Box Plot Visualization\n",
        "def plot_boxplots(data, numerical_columns=None):\n",
        "    \"\"\"\n",
        "    Plot box plots for numerical features in the dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - data: pandas DataFrame, the dataset\n",
        "    - numerical_columns: list of columns to include (default is all numeric columns)\n",
        "    \"\"\"\n",
        "    if numerical_columns is None:\n",
        "        # Automatically detect numeric columns\n",
        "        numerical_columns = data.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # Plot each numeric feature in a box plot\n",
        "    for column in numerical_columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.boxplot(x=data[column], color='skyblue', flierprops={'markerfacecolor': 'red', 'marker': 'o'})\n",
        "        plt.title(f'Box Plot of {column}')\n",
        "        plt.xlabel(column)\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.show()\n",
        "        space()\n",
        "\n",
        "# Automatically detect numeric columns\n",
        "numerical_features = data.select_dtypes(include=['number']).columns\n",
        "\n",
        "# Plot box plots for all numeric features\n",
        "plot_boxplots(data, numerical_columns=numerical_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EF2MdjRCUFXN"
      },
      "source": [
        "# Construction and plotting of the graph"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPyltQugZQt4"
      },
      "source": [
        "We now present the graph study over 4 features: distance, luminosity, age and mass. These quatities permits to go into details about charachteristics of the stellar dataset.\n",
        "\n",
        "Part of the code is blocked since the related analysis does not give any relevant information in our case, but could be useful in others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnwytgn4uJ_d"
      },
      "outputs": [],
      "source": [
        "# Additional useful libraries for the graph analysis\n",
        "from joblib import Parallel, delayed\n",
        "from networkx.algorithms.community import modularity\n",
        "from networkx.algorithms.community import louvain_communities\n",
        "from scipy.stats import spearmanr\n",
        "from scipy.spatial import KDTree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaPxlJe3naA-"
      },
      "outputs": [],
      "source": [
        "#### Definition of the functions ####\n",
        "\n",
        "# Function for plottting the degree of distribution\n",
        "def plot_degree_distribution(graph, title):\n",
        "  degree_sequence = [d for n, d in graph.degree()]\n",
        "\n",
        "  # Calcola il numero di bin usando la regola di Freedman-Diaconis\n",
        "  q1 = np.percentile(degree_sequence, 25)  # Primo quartile\n",
        "  q3 = np.percentile(degree_sequence, 75)  # Terzo quartile\n",
        "  iqr = q3 - q1  # Intervallo interquartile\n",
        "\n",
        "  bin_width = 2 * iqr / len(degree_sequence)**(1/3)  # Larghezza del bin secondo Freedman-Diaconis\n",
        "  bin_count = int((max(degree_sequence) - min(degree_sequence)) / bin_width) if bin_width > 0 else 10\n",
        "\n",
        "  plt.hist(degree_sequence, bins=10, color='orange', edgecolor='black', density=True)\n",
        "  plt.title(title)\n",
        "  plt.xlabel('degree_sequence')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "\n",
        "# Function for plottting the degree of centrality\n",
        "def plot_degree_centrality(graph, title):\n",
        "  degree_centrality = nx.degree_centrality(graph)\n",
        "  plt.figure(figsize=(6, 5))\n",
        "  nodes = nx.draw(graph, node_color=list(degree_centrality.values()),\n",
        "                  node_size=100, cmap=plt.cm.plasma, edge_color='gray')\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "# Function for plottting the clustering of louvain\n",
        "def plot_louvain_clusters(graph, title):\n",
        "  partition = community_louvain.best_partition(graph)\n",
        "  pos = nx.spring_layout(graph)\n",
        "  plt.figure(figsize=(6, 5))\n",
        "  nx.draw(graph, pos, node_color=list(partition.values()),\n",
        "          node_size=50, cmap=plt.cm.jet, with_labels=False)\n",
        "  plt.title(title)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# Analyzes a list of graphs based on thresholds and star features\n",
        "def analyze_graphs(graphs, thresholds, data):\n",
        "\n",
        "    for idx, G in enumerate(graphs):\n",
        "        print(f\"Analysis for Graph with Threshold {thresholds[idx]:.2f}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Compute and print graph density\n",
        "        density = nx.density(G)\n",
        "        print(f\"Graph Density: {density:.6f}\")\n",
        "\n",
        "        # Compute and print average degree centrality\n",
        "        degree_centrality = nx.degree_centrality(G)\n",
        "        avg_centrality = sum(degree_centrality.values()) / len(degree_centrality)\n",
        "        print(f\"Average Degree Centrality: {avg_centrality:.4f}\")\n",
        "\n",
        "        # Identify the node with the highest degree centrality\n",
        "        top_node = max(degree_centrality, key=degree_centrality.get)\n",
        "        print(f\"Top Node (ID: {top_node}) Degree Centrality: {degree_centrality[top_node]:.4f}\")\n",
        "\n",
        "        '''\n",
        "        nodes = list(G.nodes())\n",
        "        clustering_coefficients = Parallel(n_jobs=-1)(\n",
        "            delayed(compute_clustering)(node, G) for node in nodes\n",
        "        )\n",
        "        avg_clustering = sum(clustering_coefficients) / len(clustering_coefficients)\n",
        "        return avg_clustering\n",
        "        print(f\"Average Clustering Coefficient: {avg_clustering:.4f}\")\n",
        "        '''\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
        "\n",
        "# Analyzes a list of graphs, looking into clusters and stellar properties\n",
        "def analyze_clusters_and_stellar_properties(graphs, thresholds, data):\n",
        "\n",
        "    for idx, G in enumerate(graphs):\n",
        "        print(f\"\\nAnalysis for Graph with Threshold {thresholds[idx]:.2f}\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Quantitative Analysis of Clusters\n",
        "        # Use Louvain community detection\n",
        "        communities = louvain_communities(G)\n",
        "        num_clusters = len(communities)\n",
        "        print(f\"Number of Clusters: {num_clusters}\")\n",
        "\n",
        "        # Compute modularity score\n",
        "        community_map = {node: cid for cid, community in enumerate(communities) for node in community}\n",
        "        mod_score = modularity(G, communities)\n",
        "        print(f\"Modularity Score: {mod_score:.4f}\")\n",
        "\n",
        "        # Assign cluster labels to nodes\n",
        "        nx.set_node_attributes(G, community_map, \"cluster\")\n",
        "\n",
        "        '''\n",
        "        # Compare with Stellar Properties\n",
        "        # Extract cluster labels and corresponding properties\n",
        "        clusters = nx.get_node_attributes(G, \"cluster\")\n",
        "        degrees = dict(G.degree())\n",
        "\n",
        "        # Prepare a DataFrame for analysis\n",
        "        features = pd.DataFrame(data)  # Ensure 'data' is a DataFrame\n",
        "        features[\"Cluster\"] = features.index.map(clusters)\n",
        "        features[\"Degree\"] = features.index.map(degrees)\n",
        "\n",
        "        # Correlate cluster labels with mass, age, and luminosity\n",
        "        for prop in [\"Mass\", \"Age\", \"Lum\"]:\n",
        "            if prop in features.columns:\n",
        "                corr, pval = spearmanr(features[\"Cluster\"], features[prop], nan_policy='omit')\n",
        "                print(f\"Spearman Correlation (Cluster vs {prop}): {corr:.4f} (p={pval:.4e})\")\n",
        "\n",
        "        # Create a 2x2 grid of subplots\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "        axes = axes.flatten()  # Flatten axes for easier iteration\n",
        "\n",
        "        # Scatter plot: Degree vs Mass colored by cluster\n",
        "        scatter = axes[0].scatter(\n",
        "            features[\"Mass\"], features[\"Degree\"], c=features[\"Cluster\"], cmap=\"viridis\", alpha=0.7, edgecolor=\"k\"\n",
        "        )\n",
        "        axes[0].set_title(f\"Mass vs Degree (Threshold: {thresholds[idx]:.2f})\")\n",
        "        axes[0].set_xlabel(\"Mass\")\n",
        "        axes[0].set_ylabel(\"Degree\")\n",
        "        axes[0].grid()\n",
        "        plt.colorbar(scatter, ax=axes[0], label=\"Cluster\")\n",
        "\n",
        "        # Scatter plot: Luminosity vs Degree colored by cluster\n",
        "        scatter = axes[1].scatter(\n",
        "            features[\"Lum\"], features[\"Degree\"], c=features[\"Cluster\"], cmap=\"plasma\", alpha=0.7, edgecolor=\"k\"\n",
        "        )\n",
        "        axes[1].set_title(f\"Luminosity vs Degree (Threshold: {thresholds[idx]:.2f})\")\n",
        "        axes[1].set_xlabel(\"Luminosity\")\n",
        "        axes[1].set_ylabel(\"Degree\")\n",
        "        axes[1].grid()\n",
        "        plt.colorbar(scatter, ax=axes[1], label=\"Cluster\")\n",
        "\n",
        "        # Scatter plot: [Fe/H] vs Degree colored by cluster\n",
        "        scatter = axes[2].scatter(\n",
        "            features[\"[Fe/H]\"], features[\"Degree\"], c=features[\"Cluster\"], cmap=\"plasma\", alpha=0.7, edgecolor=\"k\"\n",
        "        )\n",
        "        axes[2].set_title(f\"[Fe/H] vs Degree (Threshold: {thresholds[idx]:.2f})\")\n",
        "        axes[2].set_xlabel(\"[Fe/H]\")\n",
        "        axes[2].set_ylabel(\"Degree\")\n",
        "        axes[2].grid()\n",
        "        plt.colorbar(scatter, ax=axes[2], label=\"Cluster\")\n",
        "\n",
        "        # Scatter plot: Rad-Flame vs Degree colored by cluster\n",
        "        scatter = axes[3].scatter(\n",
        "            features[\"Rad-Flame\"], features[\"Degree\"], c=features[\"Cluster\"], cmap=\"plasma\", alpha=0.7, edgecolor=\"k\"\n",
        "        )\n",
        "        axes[3].set_title(f\"Rad-Flame vs Degree (Threshold: {thresholds[idx]:.2f})\")\n",
        "        axes[3].set_xlabel(\"Rad-Flame\")\n",
        "        axes[3].set_ylabel(\"Degree\")\n",
        "        axes[3].grid()\n",
        "        plt.colorbar(scatter, ax=axes[3], label=\"Cluster\")\n",
        "\n",
        "        # Adjust layout\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        '''\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "# Function to compute edge weights based on different methods\n",
        "def compute_weight(method, lum_diff, mass_diff, age_diff, lum_scale, mass_scale, age_scale):\n",
        "    \"\"\"Computes edge weight based on chosen method.\"\"\"\n",
        "    if method == \"exponential\":\n",
        "        return np.exp(-((lum_diff / lum_scale)**2 + (mass_diff / mass_scale)**2 + (age_diff / age_scale)**2))\n",
        "\n",
        "    elif method == \"inverse_euclidean\":\n",
        "        return 1 / (1 + np.sqrt(lum_diff**2 + mass_diff**2 + age_diff**2))\n",
        "\n",
        "    elif method == \"cosine_similarity\":\n",
        "        vec1 = np.array([lum_diff, mass_diff, age_diff])\n",
        "        vec2 = np.array([lum_scale, mass_scale, age_scale])\n",
        "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2) + 1e-6)  # Avoid division by zero\n",
        "\n",
        "    elif method == \"manhattan\":\n",
        "        return 1 - ((lum_diff / lum_scale) + (mass_diff / mass_scale) + (age_diff / age_scale))\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unknown weight method!\")\n",
        "\n",
        "# function to visualized the graoh in different ways\n",
        "def visualize_graph_layouts(graph):\n",
        "    # Chosen layout\n",
        "    layouts = {\n",
        "        \"Spring Layout\": nx.spring_layout,\n",
        "        \"Circular Layout\": nx.circular_layout,\n",
        "        \"Random Layout\": nx.random_layout,\n",
        "        \"Shell Layout\": nx.shell_layout,\n",
        "        \"Kamada-Kawai Layout\": nx.kamada_kawai_layout,\n",
        "        \"Spectral Layout\": nx.spectral_layout\n",
        "    }\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Show all the layouts\n",
        "    for ax, (layout_name, layout_func) in zip(axes, layouts.items()):\n",
        "        pos = layout_func(graph)  # Evaluate node's positions\n",
        "        nx.draw(\n",
        "            graph,\n",
        "            pos,\n",
        "            ax=ax,\n",
        "            with_labels=False,\n",
        "            node_size=80,\n",
        "            node_color='deepskyblue',\n",
        "            edge_color='gray',\n",
        "            alpha=0.7\n",
        "        )\n",
        "        ax.set_title(layout_name, fontsize=14)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoN-rKWWF2cc"
      },
      "source": [
        "### All together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wH_G65R6Keij"
      },
      "source": [
        "#### All together: no weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvKmLiKKF43e"
      },
      "outputs": [],
      "source": [
        "# Define thresholds\n",
        "distance_thresholds = [500, 1500, 3500, 5000]  # parsecs\n",
        "luminosity_thresholds = [5, 50, 150, 500]  # Log-scaled solar units\n",
        "mass_thresholds = [2.0, 2.5, 3.0, 3.5]  # Focused on mean  1 std dev\n",
        "age_thresholds = [0.2, 0.3, 0.4, 0.5]  # Focused around 1 std dev\n",
        "\n",
        "# Create a KDTree for efficient nearest neighbor search based on spatial distances\n",
        "kdtree = KDTree(data[['RA_ICRS', 'DE_ICRS', 'Dist']].values)\n",
        "\n",
        "# Create a figure with 2x2 subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "# Generate the graphs based on distance, luminosity, mass, and age thresholds\n",
        "distance_graphs = []\n",
        "for idx, threshold in enumerate(distance_thresholds):\n",
        "    G_distance = nx.Graph()\n",
        "\n",
        "    # Adding nodes (stars with attributes like Age, Distance, Luminosity, Mass)\n",
        "    for index, row in data.iterrows():\n",
        "        G_distance.add_node(\n",
        "            index,\n",
        "            Dist=row['Dist'],  # Primary feature\n",
        "            RA=row['RA_ICRS'],  # Right Ascension\n",
        "            DE=row['DE_ICRS'],  # Declination\n",
        "            Lum=row['Lum'],  # Luminosity\n",
        "            Mass=row['Mass'],  # Mass\n",
        "            Age=row['Age']  # Age\n",
        "        )\n",
        "\n",
        "    # Finding neighbors within the threshold using KDTree\n",
        "    for i in range(len(data)):\n",
        "        indices = kdtree.query_ball_point(data.iloc[i][['RA_ICRS', 'DE_ICRS', 'Dist']].values, threshold)\n",
        "        for j in indices:\n",
        "            if i < j:  # Avoid duplicate edges\n",
        "                lum_diff = abs(data.iloc[i]['Lum'] - data.iloc[j]['Lum'])\n",
        "                mass_diff = abs(data.iloc[i]['Mass'] - data.iloc[j]['Mass'])\n",
        "                age_diff = abs(data.iloc[i]['Age'] - data.iloc[j]['Age'])\n",
        "\n",
        "                # Condition: distance, luminosity, mass, and age differences within thresholds\n",
        "                if (\n",
        "                    lum_diff < luminosity_thresholds[idx % len(luminosity_thresholds)] and\n",
        "                    mass_diff < mass_thresholds[idx % len(mass_thresholds)] and\n",
        "                    age_diff < age_thresholds[idx % len(age_thresholds)]\n",
        "                ):\n",
        "                    G_distance.add_edge(i, j)\n",
        "\n",
        "    # Append the graph to the list\n",
        "    distance_graphs.append(G_distance)\n",
        "\n",
        "    # Visualizing the graph on the corresponding subplot\n",
        "    nx.draw(\n",
        "        G_distance,\n",
        "        ax=axes[idx],\n",
        "        with_labels=False,\n",
        "        node_size=20,\n",
        "        node_color='deepskyblue',\n",
        "        edge_color='gray',\n",
        "        alpha=0.7\n",
        "    )\n",
        "    axes[idx].set_title(f\"Threshold: {round(threshold, 2)} parsec, Lum: {luminosity_thresholds[idx % len(luminosity_thresholds)]}, \"\n",
        "                        f\"Mass: {mass_thresholds[idx % len(mass_thresholds)]}, Age: {age_thresholds[idx % len(age_thresholds)]}\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CAQRryoKr_v"
      },
      "source": [
        "#### All together: Weighted graph (Exponential weight chosen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDgjROm4bwGR"
      },
      "outputs": [],
      "source": [
        "# Define the weight method to use: 'exponential', 'inverse_euclidean', 'cosine_similarity', or 'manhattan'\n",
        "weight_method = \"exponential\"\n",
        "\n",
        "# Define thresholds\n",
        "distance_thresholds = [500, 1500, 3500, 5000]  # parsecs\n",
        "luminosity_thresholds = [5, 50, 150, 500]  # Log-scaled solar units\n",
        "mass_thresholds = [2.0, 2.5, 3.0, 3.5]  # Focused on mean  1 std dev\n",
        "age_thresholds = [0.2, 0.3, 0.4, 0.5]  # Focused around 1 std dev\n",
        "\n",
        "# Create a KDTree for efficient nearest neighbor search based on spatial distances\n",
        "kdtree = KDTree(data[['RA_ICRS', 'DE_ICRS', 'Dist']].values)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "# Generate the graphs based on distance, luminosity, mass, and age thresholds\n",
        "distance_graphs = []\n",
        "for idx, threshold in enumerate(distance_thresholds):\n",
        "    G_distance = nx.Graph()\n",
        "\n",
        "    # Adding nodes (stars with attributes like Age, Distance, Luminosity, Mass)\n",
        "    for index, row in data.iterrows():\n",
        "        G_distance.add_node(\n",
        "            index,\n",
        "            Dist=row['Dist'],\n",
        "            RA=row['RA_ICRS'],\n",
        "            DE=row['DE_ICRS'],\n",
        "            Lum=row['Lum'],\n",
        "            Mass=row['Mass'],\n",
        "            Age=row['Age']\n",
        "        )\n",
        "\n",
        "    # Finding neighbors within the threshold using KDTree\n",
        "    for i in range(len(data)):\n",
        "        indices = kdtree.query_ball_point(data.iloc[i][['RA_ICRS', 'DE_ICRS', 'Dist']].values, threshold)\n",
        "        for j in indices:\n",
        "            if i < j:  # Avoid duplicate edges\n",
        "                lum_diff = abs(data.iloc[i]['Lum'] - data.iloc[j]['Lum'])\n",
        "                mass_diff = abs(data.iloc[i]['Mass'] - data.iloc[j]['Mass'])\n",
        "                age_diff = abs(data.iloc[i]['Age'] - data.iloc[j]['Age'])\n",
        "\n",
        "                # Compute weight based on selected method\n",
        "                weight = compute_weight(weight_method, lum_diff, mass_diff, age_diff,\n",
        "                                        luminosity_thresholds[-1], mass_thresholds[-1], age_thresholds[-1])\n",
        "\n",
        "                # Condition: distance, luminosity, mass, and age within thresholds\n",
        "                if (\n",
        "                    lum_diff < luminosity_thresholds[idx % len(luminosity_thresholds)] and\n",
        "                    mass_diff < mass_thresholds[idx % len(mass_thresholds)] and\n",
        "                    age_diff < age_thresholds[idx % len(age_thresholds)]\n",
        "                ):\n",
        "                    G_distance.add_edge(i, j, weight=weight)  # Store weight in the edge\n",
        "\n",
        "    # Append the graph to the list\n",
        "    distance_graphs.append(G_distance)\n",
        "\n",
        "    # Extract edge weights for visualization\n",
        "    edges, weights = zip(*nx.get_edge_attributes(G_distance, 'weight').items()) if len(G_distance.edges) > 0 else ([], [])\n",
        "\n",
        "    # Visualizing the graph on the corresponding subplot\n",
        "    pos = nx.spring_layout(G_distance, seed=42)  # Use force-directed layout\n",
        "    nx.draw(\n",
        "        G_distance,\n",
        "        pos,\n",
        "        ax=axes[idx],\n",
        "        with_labels=False,\n",
        "        node_size=20,\n",
        "        node_color='deepskyblue',\n",
        "        edge_color=weights,\n",
        "        edge_cmap=plt.cm.Blues,\n",
        "        alpha=0.7\n",
        "    )\n",
        "    axes[idx].set_title(f\"Thresholds: {round(threshold, 2)} parsec, {luminosity_thresholds[idx % len(luminosity_thresholds)]} s.u., \"\n",
        "                        f\" {mass_thresholds[idx % len(mass_thresholds)]}s.m.,{age_thresholds[idx % len(age_thresholds)]} Gy\", fontsize=13)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZRzrDk8LYm4"
      },
      "source": [
        "#### All together: generic layout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klSnFfmuUxLX"
      },
      "outputs": [],
      "source": [
        "visualize_graph_layouts(G_distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwVxtuUYLnPw"
      },
      "source": [
        "#### All together: mixed spring and spectral"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dleB49LqqRe6"
      },
      "outputs": [],
      "source": [
        "# Step 1: Spectral Layout for Large-Scale Structure\n",
        "pos_spectral = nx.spectral_layout(G_distance)\n",
        "\n",
        "# Step 2: Spring Layout for Fine-Grained Adjustments\n",
        "pos_refined = nx.spring_layout(G_distance, pos=pos_spectral, seed=42)\n",
        "\n",
        "# Plot Final Network\n",
        "plt.figure(figsize=(8, 8))\n",
        "nx.draw(G_distance, pos_refined, node_size=5, edge_color='gray', alpha=0.5)\n",
        "plt.title(\"Refined Galactic Structure Layout (Spectral + Spring)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdtFnaTt0i2Y"
      },
      "source": [
        "#### Analysis graph and clusters: All together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gery8eCt0i2n"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print: graph density, average degree centrality, top node ID and its degree of centrality\n",
        "analyze_graphs(distance_graphs, distance_thresholds, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lfs2QiPX0i2p"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print the cluster's properties\n",
        "analyze_clusters_and_stellar_properties(distance_graphs, distance_thresholds, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yaTNw6fT0i2q"
      },
      "source": [
        "#### Plot of degree of distribution, centrality and louvain clustering for all the features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsdZkqgK0i2q"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Distribution\n",
        "for idx, graph in enumerate(distance_graphs):\n",
        "    title = f\"{round(distance_thresholds[idx], 2)} parsec, {luminosity_thresholds[idx % len(luminosity_thresholds)]} s.u., {mass_thresholds[idx % len(mass_thresholds)]}s.m., {age_thresholds[idx % len(age_thresholds)]} Gy\"\n",
        "\n",
        "    # Degree Distribution\n",
        "    plot_degree_distribution(graph, f\"Degree Distribution: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEuyzOWh0i2q"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Centrality\n",
        "for idx, graph in enumerate(distance_graphs):\n",
        "    title = f\"{round(distance_thresholds[idx], 2)} parsec, {luminosity_thresholds[idx % len(luminosity_thresholds)]} s.u., {mass_thresholds[idx % len(mass_thresholds)]}s.m., {age_thresholds[idx % len(age_thresholds)]} Gy\"\n",
        "\n",
        "    plot_degree_centrality(graph, f\"Degree Centrality: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfw9N1PT0i2s"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Louvain Clustering\n",
        "for idx, graph in enumerate(distance_graphs):\n",
        "    title = f\"{round(distance_thresholds[idx], 2)} parsec, {luminosity_thresholds[idx % len(luminosity_thresholds)]} s.u., {mass_thresholds[idx % len(mass_thresholds)]}s.m., {age_thresholds[idx % len(age_thresholds)]} Gy\"\n",
        "\n",
        "    # Louvain Clustering\n",
        "    plot_louvain_clusters(graph, f\"Louvain Clusters: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-a4Alrjs4MB"
      },
      "source": [
        "### Distance and luminosity together"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute inverse Euclidean weight\n",
        "def inverse_euclidean_weight(dist_diff, lum_diff):\n",
        "    return 1 / (1 + np.sqrt(dist_diff**2 + lum_diff**2))  # Normalize weight"
      ],
      "metadata": {
        "id": "eM5XoePepOvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCXRwa03s8ct"
      },
      "outputs": [],
      "source": [
        "distance_thresholds = [500, 1500, 3500, 5000]  # Parsecs\n",
        "luminosity_thresholds = [5, 50, 150, 500]  # Solar luminosities\n",
        "\n",
        "# Create a KDTree for efficient nearest neighbor search based on spatial distances\n",
        "kdtree = KDTree(data[['RA_ICRS', 'DE_ICRS', 'Dist']].values)\n",
        "\n",
        "# **Create a figure and axes for subplots**\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "# Generate the graphs based on distance and luminosity thresholds\n",
        "distance_graphs = []\n",
        "for idx, threshold in enumerate(distance_thresholds):\n",
        "    G_distance = nx.Graph()\n",
        "\n",
        "    # Adding nodes (stars with attributes: Distance, Luminosity)\n",
        "    for index, row in data.iterrows():\n",
        "        G_distance.add_node(\n",
        "            index,\n",
        "            Dist=row['Dist'],  # Distance\n",
        "            Lum=row['Lum'],  # Luminosity\n",
        "        )\n",
        "\n",
        "    # Finding neighbors within the threshold using KDTree\n",
        "    for i in range(len(data)):\n",
        "        indices = kdtree.query_ball_point(data.iloc[i][['RA_ICRS', 'DE_ICRS', 'Dist']].values, threshold)\n",
        "        for j in indices:\n",
        "            if i < j:  # Avoid duplicate edges\n",
        "                dist_diff = abs(data.iloc[i]['Dist'] - data.iloc[j]['Dist'])\n",
        "                lum_diff = abs(data.iloc[i]['Lum'] - data.iloc[j]['Lum'])\n",
        "\n",
        "                # Compute inverse Euclidean weight\n",
        "                weight = inverse_euclidean_weight(dist_diff, lum_diff)\n",
        "\n",
        "                # Condition: distance and luminosity within thresholds\n",
        "                if (\n",
        "                    lum_diff < luminosity_thresholds[idx % len(luminosity_thresholds)]\n",
        "                ):\n",
        "                    G_distance.add_edge(i, j, weight=weight)  # Store weight in the edge\n",
        "\n",
        "    # Append the graph to the list\n",
        "    distance_graphs.append(G_distance)\n",
        "\n",
        "    # Extract edge weights for visualization\n",
        "    edges, weights = zip(*nx.get_edge_attributes(G_distance, 'weight').items()) if len(G_distance.edges) > 0 else ([], [])\n",
        "\n",
        "    # Visualizing the graph on the corresponding subplot\n",
        "    pos = nx.spring_layout(G_distance, seed=42)  # Use force-directed layout\n",
        "    nx.draw(\n",
        "        G_distance,\n",
        "        pos,\n",
        "        ax=axes[idx],\n",
        "        with_labels=False,\n",
        "        node_size=20,\n",
        "        node_color='deepskyblue',\n",
        "        edge_color=weights,\n",
        "        edge_cmap=plt.cm.Blues,  # Edge colors represent weight strength\n",
        "        alpha=0.7\n",
        "    )\n",
        "    axes[idx].set_title(f\"Thresholds: {round(threshold, 2)} parsec, {luminosity_thresholds[idx % len(luminosity_thresholds)]} s.u.\",\n",
        "                        fontsize=13)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PclqNp5ntTDZ"
      },
      "source": [
        "#### Analysis graph and clusters: Distance and luminosity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZDt-EMttTDg"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print: graph density, average degree centrality, top node ID and its degree of centrality\n",
        "analyze_graphs(distance_graphs, distance_thresholds, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pa9g_s53tTDi"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print the cluster's properties\n",
        "analyze_clusters_and_stellar_properties(distance_graphs, distance_thresholds, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArTGmerjtTDj"
      },
      "source": [
        "#### Plot of degree of distribution, centrality and louvain clustering for luminosity and distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kgszfOKtTDk"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Distribution\n",
        "for idx, graph in enumerate(distance_graphs):\n",
        "    title = f\"{round(distance_thresholds[idx],2)} parsec and {round(luminosity_thresholds[idx],2)} s.u.\"\n",
        "\n",
        "    # Degree Distribution\n",
        "    plot_degree_distribution(graph, f\"Degree of distribution: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNSdjXGRtTDl"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Centrality\n",
        "for idx, graph in enumerate(distance_graphs):\n",
        "    title = f\"{round(distance_thresholds[idx],2)} parsec and {round(luminosity_thresholds[idx],2)} s.u.\"\n",
        "\n",
        "    plot_degree_centrality(graph, f\"Degree Centrality: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4CGzRh2ItTDo"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Louvain Clustering\n",
        "for idx, graph in enumerate(distance_graphs):\n",
        "    title = f\"{round(distance_thresholds[idx],2)} parsec and {round(luminosity_thresholds[idx],2)} s. u.\"\n",
        "\n",
        "    # Louvain Clustering\n",
        "    plot_louvain_clusters(graph, f\"Louvain Clusters: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuVUK4T7C8jq"
      },
      "source": [
        "## Age and mass together\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Age and mass together: Exp weight\n"
      ],
      "metadata": {
        "id": "McU4oEwLpb6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Exponential Similarity\n",
        "def exponential_similarity(mass1, age1, mass2, age2, T_mass, T_age):\n",
        "    \"\"\"Computes the Exponential Similarity between two stars based on Mass and Age.\"\"\"\n",
        "    mass_diff = abs(mass1 - mass2) / T_mass\n",
        "    age_diff = abs(age1 - age2) / T_age\n",
        "\n",
        "    similarity = np.exp(- (mass_diff ** 2) - (age_diff ** 2))\n",
        "\n",
        "    return similarity  # Higher means more similar"
      ],
      "metadata": {
        "id": "mNdynCQGpWB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMaVMRidC3sR"
      },
      "outputs": [],
      "source": [
        "# Define Mass and Age thresholds\n",
        "mass_thresholds = [2.0, 2.5, 3.0, 3.5]  # Focused on mean  1 std dev\n",
        "age_thresholds = [0.2, 0.3, 0.4, 0.5]  # Focused around 1 std dev\n",
        "\n",
        "# Create a KDTree for efficient nearest neighbor search based on Mass and Age\n",
        "kdtree = KDTree(data[['Age', 'Mass']].values)\n",
        "\n",
        "# Create a figure and axes for subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "# Generate the graphs based on Age and Mass thresholds\n",
        "age_mass_graphs = []\n",
        "for idx, threshold in enumerate(age_thresholds):  # Using age thresholds as reference\n",
        "    G_age_mass = nx.Graph()\n",
        "\n",
        "    # Adding nodes (stars with attributes: Age, Mass)\n",
        "    for index, row in data.iterrows():\n",
        "        G_age_mass.add_node(\n",
        "            index,\n",
        "            Mass=row['Mass'],\n",
        "            Age=row['Age']\n",
        "        )\n",
        "\n",
        "    # Finding neighbors within the threshold using KDTree\n",
        "    for i in range(len(data)):\n",
        "        indices = kdtree.query_ball_point(data.iloc[i][['Age', 'Mass']].values, threshold)\n",
        "        for j in indices:\n",
        "            if i < j:  # Avoid duplicate edges\n",
        "                mass1, age1 = data.iloc[i]['Mass'], data.iloc[i]['Age']\n",
        "                mass2, age2 = data.iloc[j]['Mass'], data.iloc[j]['Age']\n",
        "\n",
        "                # Compute Exponential Similarity\n",
        "                weight = exponential_similarity(mass1, age1, mass2, age2,\n",
        "                                                T_mass=mass_thresholds[-1],\n",
        "                                                T_age=age_thresholds[-1])\n",
        "\n",
        "                # Condition: age and mass within thresholds\n",
        "                if (\n",
        "                    abs(mass1 - mass2) < mass_thresholds[idx % len(mass_thresholds)] and\n",
        "                    abs(age1 - age2) < age_thresholds[idx % len(age_thresholds)]\n",
        "                ):\n",
        "                    G_age_mass.add_edge(i, j, weight=weight)  # Store weight in the edge\n",
        "\n",
        "    # Append the graph to the list\n",
        "    age_mass_graphs.append(G_age_mass)\n",
        "\n",
        "    # Extract edge weights for visualization\n",
        "    edges, weights = zip(*nx.get_edge_attributes(G_age_mass, 'weight').items()) if len(G_age_mass.edges) > 0 else ([], [])\n",
        "\n",
        "    # Visualizing the graph on the corresponding subplot\n",
        "    pos = nx.spring_layout(G_age_mass, seed=42)  # Use force-directed layout\n",
        "    nx.draw(\n",
        "        G_age_mass,\n",
        "        pos,\n",
        "        ax=axes[idx],\n",
        "        with_labels=False,\n",
        "        node_size=20,\n",
        "        node_color='deepskyblue',\n",
        "        edge_color=weights,\n",
        "        edge_cmap=plt.cm.Blues,  # Edge colors represent similarity strength\n",
        "        alpha=0.7\n",
        "    )\n",
        "    axes[idx].set_title(f\"Thresholds: {round(threshold, 2)} Gy, {mass_thresholds[idx % len(mass_thresholds)]} s.m.\",fontsize=13)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Age and mass together: Euclidean weight\n"
      ],
      "metadata": {
        "id": "EPewdpXfpkZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Inverse Euclidean Weight\n",
        "def inverse_euclidean_weight(mass1, age1, mass2, age2):\n",
        "    \"\"\"Computes the Inverse Euclidean Weight between two stars based on Mass and Age.\"\"\"\n",
        "    mass_diff = mass1 - mass2\n",
        "    age_diff = age1 - age2\n",
        "\n",
        "    distance = np.sqrt(mass_diff**2 + age_diff**2)\n",
        "\n",
        "    weight = 1 / (1 + distance)  # Normalize weight (closer values have higher weights)\n",
        "    return weight"
      ],
      "metadata": {
        "id": "5pD-0fMdppn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Mass and Age thresholds\n",
        "mass_thresholds = [2.0, 2.5, 3.0, 3.5]  # Focused on mean  1 std dev\n",
        "age_thresholds = [0.2, 0.3, 0.4, 0.5]  # Focused around 1 std dev\n",
        "\n",
        "# Create a KDTree for efficient nearest neighbor search based on Mass and Age\n",
        "kdtree = KDTree(data[['Age', 'Mass']].values)\n",
        "\n",
        "# Create a figure and axes for subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "# Generate the graphs based on Age and Mass thresholds\n",
        "age_mass_graphs = []\n",
        "for idx, threshold in enumerate(age_thresholds):  # Using age thresholds as reference\n",
        "    G_age_mass = nx.Graph()\n",
        "\n",
        "    # Adding nodes (stars with attributes: Age, Mass)\n",
        "    for index, row in data.iterrows():\n",
        "        G_age_mass.add_node(\n",
        "            index,\n",
        "            Mass=row['Mass'],\n",
        "            Age=row['Age']\n",
        "        )\n",
        "\n",
        "    # Finding neighbors within the threshold using KDTree\n",
        "    for i in range(len(data)):\n",
        "        indices = kdtree.query_ball_point(data.iloc[i][['Age', 'Mass']].values, threshold)\n",
        "        for j in indices:\n",
        "            if i < j:  # Avoid duplicate edges\n",
        "                mass1, age1 = data.iloc[i]['Mass'], data.iloc[i]['Age']\n",
        "                mass2, age2 = data.iloc[j]['Mass'], data.iloc[j]['Age']\n",
        "\n",
        "                # Compute Inverse Euclidean Weight\n",
        "                weight = inverse_euclidean_weight(mass1, age1, mass2, age2)\n",
        "\n",
        "                # Condition: age and mass within thresholds\n",
        "                if (\n",
        "                    abs(mass1 - mass2) < mass_thresholds[idx % len(mass_thresholds)] and\n",
        "                    abs(age1 - age2) < age_thresholds[idx % len(age_thresholds)]\n",
        "                ):\n",
        "                    G_age_mass.add_edge(i, j, weight=weight)  # Store weight in the edge\n",
        "\n",
        "    # Append the graph to the list\n",
        "    age_mass_graphs.append(G_age_mass)\n",
        "\n",
        "    # Extract edge weights for visualization\n",
        "    edges, weights = zip(*nx.get_edge_attributes(G_age_mass, 'weight').items()) if len(G_age_mass.edges) > 0 else ([], [])\n",
        "\n",
        "    # Visualizing the graph on the corresponding subplot\n",
        "    pos = nx.spring_layout(G_age_mass, seed=42)  # Use force-directed layout\n",
        "    nx.draw(\n",
        "        G_age_mass,\n",
        "        pos,\n",
        "        ax=axes[idx],\n",
        "        with_labels=False,\n",
        "        node_size=20,\n",
        "        node_color='deepskyblue',\n",
        "        edge_color=weights,\n",
        "        edge_cmap=plt.cm.Blues,  # Edge colors represent similarity strength\n",
        "        alpha=0.7\n",
        "    )\n",
        "    axes[idx].set_title(f\"Thresholds: {round(threshold, 2)} Gy, {mass_thresholds[idx % len(mass_thresholds)]} s.m.\",fontsize=13)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4UkOPVGNBS4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Age and mass together: Spectral layout and exp weight"
      ],
      "metadata": {
        "id": "AlHpmn8VptCp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute Exponential Similarity\n",
        "def exponential_similarity(mass1, age1, mass2, age2, T_mass, T_age):\n",
        "    \"\"\"Computes the Exponential Similarity between two stars based on Mass and Age.\"\"\"\n",
        "    mass_diff = abs(mass1 - mass2) / T_mass\n",
        "    age_diff = abs(age1 - age2) / T_age\n",
        "\n",
        "    similarity = np.exp(- (mass_diff ** 2) - (age_diff ** 2))\n",
        "\n",
        "    return similarity  # Higher means more similar"
      ],
      "metadata": {
        "id": "6BK0nIPeqCvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Mass and Age thresholds\n",
        "mass_thresholds = [2.0, 2.5, 3.0, 3.5]  # Focused on mean  1 std dev\n",
        "age_thresholds = [0.2, 0.3, 0.4, 0.5]  # Focused around 1 std dev\n",
        "\n",
        "# Create a KDTree for efficient nearest neighbor search based on Mass and Age\n",
        "kdtree = KDTree(data[['Age', 'Mass']].values)\n",
        "\n",
        "# Create a figure and axes for subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "# Generate the graphs based on Age and Mass thresholds\n",
        "age_mass_graphs = []\n",
        "for idx, threshold in enumerate(age_thresholds):  # Using age thresholds as reference\n",
        "    G_age_mass = nx.Graph()\n",
        "\n",
        "    # Adding nodes (stars with attributes: Age, Mass)\n",
        "    for index, row in data.iterrows():\n",
        "        G_age_mass.add_node(\n",
        "            index,\n",
        "            Mass=row['Mass'],\n",
        "            Age=row['Age']\n",
        "        )\n",
        "\n",
        "    # Finding neighbors within the threshold using KDTree\n",
        "    for i in range(len(data)):\n",
        "        indices = kdtree.query_ball_point(data.iloc[i][['Age', 'Mass']].values, threshold)\n",
        "        for j in indices:\n",
        "            if i < j:  # Avoid duplicate edges\n",
        "                mass1, age1 = data.iloc[i]['Mass'], data.iloc[i]['Age']\n",
        "                mass2, age2 = data.iloc[j]['Mass'], data.iloc[j]['Age']\n",
        "\n",
        "                # Compute Exponential Similarity\n",
        "                weight = exponential_similarity(mass1, age1, mass2, age2,\n",
        "                                                T_mass=mass_thresholds[-1],\n",
        "                                                T_age=age_thresholds[-1])\n",
        "\n",
        "                # Condition: age and mass within thresholds\n",
        "                if (\n",
        "                    abs(mass1 - mass2) < mass_thresholds[idx % len(mass_thresholds)] and\n",
        "                    abs(age1 - age2) < age_thresholds[idx % len(age_thresholds)]\n",
        "                ):\n",
        "                    G_age_mass.add_edge(i, j, weight=weight)  # Store weight in the edge\n",
        "\n",
        "    # Append the graph to the list\n",
        "    age_mass_graphs.append(G_age_mass)\n",
        "\n",
        "    # Extract edge weights for visualization\n",
        "    edges, weights = zip(*nx.get_edge_attributes(G_age_mass, 'weight').items()) if len(G_age_mass.edges) > 0 else ([], [])\n",
        "\n",
        "    # Visualizing the graph on the corresponding subplot\n",
        "    pos = nx.spectral_layout(G_age_mass)  # Use force-directed layout\n",
        "    nx.draw(\n",
        "        G_age_mass,\n",
        "        pos,\n",
        "        ax=axes[idx],\n",
        "        with_labels=False,\n",
        "        node_size=20,\n",
        "        node_color='deepskyblue',\n",
        "        edge_color=weights,\n",
        "        edge_cmap=plt.cm.Blues,  # Edge colors represent similarity strength\n",
        "        alpha=0.7\n",
        "    )\n",
        "    axes[idx].set_title(f\"Thresholds: {round(threshold, 2)} Gy, {mass_thresholds[idx % len(mass_thresholds)]} s.m.\",fontsize=13)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "74jAcgJd7AiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8ue5DK1DH7J"
      },
      "source": [
        "### Analysis graph and clusters: Distance and luminosity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cErdkXcbC3sd"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print: graph density, average degree centrality, top node ID and its degree of centrality\n",
        "analyze_graphs(age_mass_graphs, age_thresholds, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s4Ti3qwBC3sg"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print the cluster's properties\n",
        "analyze_clusters_and_stellar_properties(age_mass_graphs, age_thresholds, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVjSUeerDH7S"
      },
      "source": [
        "### Plot of degree of distribution, centrality and louvain clustering for luminosity and distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAP2hILfC3sj"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Distribution\n",
        "for idx, graph in enumerate(age_mass_graphs):\n",
        "    title = f\"{age_thresholds[idx % len(mass_thresholds)]} Gy, {mass_thresholds[idx % len(mass_thresholds)]} s.m.\"\n",
        "\n",
        "    # Degree Distribution\n",
        "    plot_degree_distribution(graph, f\"Degree Distribution: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkKVovMUC3sm"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Centrality\n",
        "for idx, graph in enumerate(age_mass_graphs):\n",
        "    title = f\"{age_thresholds[idx % len(mass_thresholds)]} Gy, {mass_thresholds[idx % len(mass_thresholds)]} s.m.\"\n",
        "\n",
        "    plot_degree_centrality(graph, f\"Degree Centrality: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edly2P2rC3sn"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Louvain Clustering\n",
        "for idx, graph in enumerate(age_mass_graphs):\n",
        "    title = f\"{age_thresholds[idx % len(mass_thresholds)]} Gy, {mass_thresholds[idx % len(mass_thresholds)]} s.m.\"\n",
        "\n",
        "    # Louvain Clustering\n",
        "    plot_louvain_clusters(graph, f\"Louvain Clusters: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVUdIbWD2YtX"
      },
      "source": [
        "## Graph of the relative distance\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cJAf9t7_ZOC"
      },
      "source": [
        "We proceed to construct the graph of the relative distances. We will use \"kdtree\" to speed up the computation and use 4 meaningful thresholds: one smaller than the mean, the mean, one little higher than the mean and one greater than the mean. This permits us to have a full view on the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX3bpqnp-Y1M",
        "outputId": "de7d4260-c1d6-4627-fbe8-a4ebb3db5bad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1765.2297997607654, 3530.459599521531, 3883.505559473684, 7060.919199043062]\n"
          ]
        }
      ],
      "source": [
        "#### Definition of the needed quantities ####\n",
        "\n",
        "# Using only the filtered distances for KDTree\n",
        "distances = data[['Dist']].values\n",
        "kdtree = KDTree(distances)\n",
        "\n",
        "# evaluate the mean distance\n",
        "mean_distance = distances.mean()\n",
        "\n",
        "# Define multiple distance thresholds for comparison\n",
        "distance_thresholds = [0.5*mean_distance, mean_distance, (mean_distance +0.10*mean_distance), 2*mean_distance]\n",
        "print(distance_thresholds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mf_xCKs4lK5t"
      },
      "outputs": [],
      "source": [
        "#### Graph construction ####\n",
        "\n",
        "# Create a figure with 2x2 subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "# Generate the graphs based on distance thresholds\n",
        "distance_graphs = []\n",
        "for idx, threshold in enumerate(distance_thresholds):\n",
        "    G_distance = nx.Graph()\n",
        "\n",
        "    # Adding nodes (stars with attributes like Age, distance, luminosity, Mass)\n",
        "    for index, row in data.iterrows():\n",
        "      G_distance.add_node(\n",
        "          index,\n",
        "          Dist=row['Dist'],        # Primary feature\n",
        "          RA=row['RA_ICRS'],       # Right Ascension for spatial context\n",
        "          DE=row['DE_ICRS'],       # Declination for spatial context\n",
        "          Lum=row['Lum'],          # Luminosity for brightness context\n",
        "          Mass=row['Mass'],        # Mass for understanding spatial clustering of massive stars\n",
        "          Age=row['Age']           # Age for evolutionary insights: older stars may populate farther regions\n",
        "      )\n",
        "\n",
        "    # Finding neighbors within the threshold using KDTree\n",
        "    for i in range(len(data)):\n",
        "        indices = kdtree.query_ball_point(distances[i], threshold)\n",
        "        for j in indices:\n",
        "            if i < j:  # Avoid duplicate edges\n",
        "                G_distance.add_edge(i, j)\n",
        "\n",
        "\n",
        "    # Append the graph to the list\n",
        "    distance_graphs.append(G_distance)\n",
        "\n",
        "    #Visualizing the graph on the corresponding subplot\n",
        "    nx.draw(\n",
        "        G_distance,\n",
        "        ax=axes[idx],\n",
        "        with_labels=False,\n",
        "        node_size=20,\n",
        "        node_color='deepskyblue',\n",
        "        edge_color='gray',\n",
        "        alpha=0.7\n",
        "    )\n",
        "    axes[idx].set_title(f\"Threshold: {round(threshold,2)} parsec\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKD5jUnAPblT"
      },
      "source": [
        "### Analysis graph and clusters: Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EX3QX9gEdZX2"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print: graph density, average degree centrality, top node ID and its degree of centrality\n",
        "analyze_graphs(distance_graphs, distance_thresholds, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHcxmgyzGaOi"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print the cluster's properties\n",
        "analyze_clusters_and_stellar_properties(distance_graphs, distance_thresholds, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTeXUCdmQTiZ"
      },
      "source": [
        "#### Plot of degree of distribution, centrality and louvain clustering for distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCN1uvFx5a7-"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Distribution\n",
        "for idx, graph in enumerate(distance_graphs):\n",
        "    title = f\"Graph with Threshold {round(distance_thresholds[idx],2)} parsec\"\n",
        "\n",
        "    # Degree Distribution\n",
        "    plot_degree_distribution(graph, f\"Degree Distribution: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmNRBG4kipwl"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Centrality\n",
        "for idx, graph in enumerate(distance_graphs):\n",
        "    title = f\"Graph with Threshold {round(distance_thresholds[idx],2)} parsec\"\n",
        "\n",
        "    plot_degree_centrality(graph, f\"Degree Centrality: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmh0b0Yr5a8I"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Louvain Clustering\n",
        "for idx, graph in enumerate(distance_graphs):\n",
        "    title = f\"Graph with Threshold {round(distance_thresholds[idx],2)} parsec\"\n",
        "\n",
        "    # Louvain Clustering\n",
        "    plot_louvain_clusters(graph, f\"Louvain Clusters: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkrNT9J92kB8"
      },
      "source": [
        "## Graph of the relative luminosities\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMPaitF5Yvfb",
        "outputId": "b1f7aefc-253e-448c-9bbe-c48b108a3ca3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[44.83322198066188, 89.66644396132376, 98.63308835745615, 179.33288792264753]\n"
          ]
        }
      ],
      "source": [
        "#### Definition of the needed quantities ####\n",
        "\n",
        "luminosities = data[['Lum']].values\n",
        "kdtree = KDTree(luminosities)\n",
        "\n",
        "# evaluate the mean luminosity\n",
        "mean_luminosity = luminosities.mean()\n",
        "\n",
        "# Define multiple luminosity thresholds for comparison\n",
        "luminosity_thresholds = [0.5*mean_luminosity,mean_luminosity, (mean_luminosity+0.10*mean_luminosity),2*mean_luminosity]\n",
        "print(luminosity_thresholds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maGwBuHKxOXy"
      },
      "outputs": [],
      "source": [
        "#### Graph construction ####\n",
        "\n",
        "# Create a figure with 2x2 subplots\n",
        "fig, axes = plt.subplots(2,2, figsize=(12, 10))\n",
        "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "# Generate the graphs based on luminosity thresholds\n",
        "luminosity_graphs = []\n",
        "for idx,threshold in enumerate(luminosity_thresholds):\n",
        "    G_luminosity = nx.Graph()\n",
        "\n",
        "    # Adding nodes\n",
        "    for index, row in data.iterrows():\n",
        "        G_luminosity.add_node(\n",
        "            index,\n",
        "            Lum=row['Lum'],         # Primary feature\n",
        "            Mass=row['Mass'],        # Stellar mass to explore its relationship with luminosity\n",
        "            Teff=row['Teff'],        # Effective temperature for stellar classification\n",
        "            Age=row['Age'],          # Age to analyze brightness evolution\n",
        "            Rad=row['Rad-Flame']           # Larger stars emit more energy\n",
        "        )\n",
        "\n",
        "    # Adding edges using KDTree\n",
        "    for i in range(len(data)):\n",
        "        indices = kdtree.query_ball_point(luminosities[i], threshold)\n",
        "        for j in indices:\n",
        "            if i < j:\n",
        "                G_luminosity.add_edge(i, j)\n",
        "\n",
        "    luminosity_graphs.append(G_luminosity)\n",
        "\n",
        "    # Visualizing the graph on the corresponding subplot\n",
        "    nx.draw(\n",
        "        G_luminosity,\n",
        "        ax=axes[idx],\n",
        "        with_labels=False,\n",
        "        node_size=20,\n",
        "        node_color='deepskyblue',\n",
        "        edge_color='gray',\n",
        "        alpha=0.7\n",
        "    )\n",
        "    axes[idx].set_title(f\"Threshold: {round(threshold,2)} in solar luminosity \", fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOm_YJaDPmRg"
      },
      "source": [
        "#### Analysis graph and clusters: Luminosity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKh2EnEPlgHv"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print: graph density, average degree centrality, top node ID and its degree of centrality\n",
        "analyze_graphs(luminosity_graphs, luminosity_thresholds, data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4z8K2LXasqt"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print the cluster's properties\n",
        "analyze_clusters_and_stellar_properties( luminosity_graphs,  luminosity_thresholds, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACbdJEn6QQiL"
      },
      "source": [
        "#### Plot of degree of distribution, centrality and louvain clustering for Luminosity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKRu0wA76EMK"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Distribution\n",
        "for idx, graph in enumerate(luminosity_graphs):\n",
        "    title = f\"Graph with Luminosity Threshold {round(luminosity_thresholds[idx],2)}\"\n",
        "\n",
        "    # Degree Distribution\n",
        "    plot_degree_distribution(graph, f\"Degree Distribution: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFwk8VNJ6EMQ"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Centrality\n",
        "for idx, graph in enumerate(luminosity_graphs):\n",
        "    title = f\"Graph with Luminosity Threshold {round(luminosity_thresholds[idx],2)}\"\n",
        "\n",
        "    # Degree Centrality\n",
        "    plot_degree_centrality(graph, f\"Degree Centrality: {title}\")\n",
        "    print('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvrBxtl46EMS"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Louvain Clustering\n",
        "for idx, graph in enumerate(luminosity_graphs):\n",
        "    title = f\"Graph with Luminosity Threshold {round(luminosity_thresholds[idx],2)}\"\n",
        "\n",
        "    # Louvain Clustering\n",
        "    plot_louvain_clusters(graph, f\"Louvain Clusters: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKn3UdFz2rNl"
      },
      "source": [
        "## Graph of the relative Ages\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1-dCiGsZDBa",
        "outputId": "4decc03d-ef57-4bd0-dc0a-781425ac6943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.14810147527910686, 0.2962029505582137, 0.3258232456140351, 0.5924059011164274]\n"
          ]
        }
      ],
      "source": [
        "#### Definition of the needed quantities ####\n",
        "\n",
        "Ages = data[['Age']].values\n",
        "kdtree = KDTree(Ages)\n",
        "\n",
        "# evaluate the mean age\n",
        "mean_age = Ages.mean()\n",
        "\n",
        "# Define multiple Age thresholds for comparison\n",
        "Age_thresholds = [0.5*mean_age, mean_age,(mean_age+0.10*mean_age), 2*mean_age]\n",
        "print(Age_thresholds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXfb7r0LxSUC"
      },
      "outputs": [],
      "source": [
        "#### Graph construction ####\n",
        "\n",
        "# Create a figure with 2x2 subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "# Generate the graphs based on Age thresholds\n",
        "Age_graphs = []\n",
        "for idx, threshold in enumerate(Age_thresholds):\n",
        "    G_Age = nx.Graph()\n",
        "\n",
        "    # Adding nodes\n",
        "    for index, row in data.iterrows():\n",
        "        G_Age.add_node(\n",
        "          index,\n",
        "          Age=row['Age'],         # Primary feature\n",
        "          Mass=row['Mass'],        # Massive stars evolve differently\n",
        "          Lum=row['Lum'],          # Brightness for evolutionary comparison\n",
        "          Metallicity=row['[Fe/H]'],  # To study star formation history\n",
        "          Rad=row['Rad-Flame']           # Radius for evolutionary stage analysis\n",
        "        )\n",
        "    # Adding edges using KDTree\n",
        "    for i in range(len(data)):\n",
        "        indices = kdtree.query_ball_point(Ages[i], threshold)\n",
        "        for j in indices:\n",
        "            if i < j:\n",
        "                G_Age.add_edge(i, j)\n",
        "\n",
        "    Age_graphs.append(G_Age)\n",
        "\n",
        "    # Visualizing the graph on the corresponding subplot\n",
        "    nx.draw(\n",
        "        G_Age,\n",
        "        ax=axes[idx],\n",
        "        with_labels=False,\n",
        "        node_size=20,\n",
        "        node_color='deepskyblue',\n",
        "        edge_color='gray',\n",
        "        alpha=0.7\n",
        "    )\n",
        "    axes[idx].set_title(f\"Threshold: {round(threshold,2)}\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()  # Adjust layout for better spacing\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZy6zPuUP2Gd"
      },
      "source": [
        "#### Analysis graph and clusters: Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQsiRKzCaMFA"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print: graph density, average degree centrality, top node ID and its degree of centrality\n",
        "analyze_graphs(Age_graphs, Age_thresholds, data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beKeOiLVwgUI"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print the cluster's properties\n",
        "analyze_clusters_and_stellar_properties(Age_graphs, Age_thresholds, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_yQZ_KxQKeO"
      },
      "source": [
        "#### Plot of degree of distribution, centrality and louvain clustering for Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_arIJZVB7Iff"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Distribution\n",
        "for idx, graph in enumerate(Age_graphs):\n",
        "    title = f\"Graph with Age Threshold {round(Age_thresholds[idx],2)}\"\n",
        "\n",
        "    # Degree Distribution\n",
        "    plot_degree_distribution(graph, f\"Degree Distribution: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AEdrfHee7Ifm"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Centrality\n",
        "for idx, graph in enumerate(Age_graphs):\n",
        "    title = f\"Graph with Age Threshold {round(Age_thresholds[idx],2)}\"\n",
        "\n",
        "    # Degree Centrality\n",
        "    plot_degree_centrality(graph, f\"Degree Centrality: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWvYXJDe7Ifn"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Louvain Clustering\n",
        "for idx, graph in enumerate(Age_graphs):\n",
        "    title = f\"Graph with Age Threshold {round(Age_thresholds[idx],2)}\"\n",
        "\n",
        "\n",
        "    # Louvain Clustering\n",
        "    plot_louvain_clusters(graph, f\"Louvain Clusters: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khPrb1Iv2y-Q"
      },
      "source": [
        "## Graph of the relative Masses\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3bRMqBAZYXW",
        "outputId": "5a9c11f8-6f9f-4c0b-83de-7eef54247c28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.3628137958532696, 2.725627591706539, 2.998190350877193, 5.451255183413078]\n"
          ]
        }
      ],
      "source": [
        "#### Definition of the needed quantities ####\n",
        "\n",
        "Masses = data[['Mass']].values\n",
        "kdtree = KDTree(Masses)\n",
        "\n",
        "# evaluate the mean mass\n",
        "mean_mass = Masses.mean()\n",
        "\n",
        "# Define multiple mass thresholds for comparison\n",
        "Mass_thresholds = [0.5*mean_mass, mean_mass, (mean_mass +0.10*mean_mass), 2* mean_mass]\n",
        "print(Mass_thresholds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "h1KCq4NvxWV3",
        "outputId": "d93ba3fa-9c71-4a08-b055-2dca7091019f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#### Graph construction ####\n",
        "\n",
        "# Create a figure with 2x2 subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "axes = axes.flatten()  # Flatten the axes array for easy iteration\n",
        "\n",
        "# Generate the graphs based on Mass thresholds\n",
        "Mass_graphs = []\n",
        "for idx,threshold in enumerate(Mass_thresholds):\n",
        "    G_Mass = nx.Graph()\n",
        "\n",
        "    # Adding nodes\n",
        "    for index, row in data.iterrows():\n",
        "        G_Mass.add_node(\n",
        "        index,\n",
        "        Mass=row['Mass'],        # Primary feature\n",
        "        Lum=row['Lum'],          # Luminosity to examine massive stars' brightness\n",
        "        Teff=row['Teff'],        # Effective temperature for classification\n",
        "        Age=row['Age'],          # Tracks stellar evolution\n",
        "        Rad=row['Rad-Flame']           # Stellar radius for physical properties\n",
        "    )\n",
        "\n",
        "    # Adding edges using KDTree\n",
        "    for i in range(len(data)):\n",
        "        indices = kdtree.query_ball_point(Masses[i], threshold)\n",
        "        for j in indices:\n",
        "            if i < j:\n",
        "                G_Mass.add_edge(i, j)\n",
        "\n",
        "    Mass_graphs.append(G_Mass)\n",
        "\n",
        "    # Visualizing the graph on the corresponding subplot\n",
        "    nx.draw(\n",
        "        G_Mass,\n",
        "        ax=axes[idx],\n",
        "        with_labels=False,\n",
        "        node_size=20,\n",
        "        node_color='deepskyblue',\n",
        "        edge_color='gray',\n",
        "        alpha=0.7\n",
        "    )\n",
        "    axes[idx].set_title(f\"Threshold: {round(threshold,2)}\", fontsize=10)\n",
        "\n",
        "plt.tight_layout()  # Adjust layout for better spacing\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLs1Hf09P4SC"
      },
      "source": [
        "#### Analysis graph and clusters: Mass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwUzzYZaaNnL"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print: graph density, average degree centrality, top node ID and its degree of centrality\n",
        "analyze_graphs(Mass_graphs, Mass_thresholds, data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJAFjEzDPdd1"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate and print the cluster's properties\n",
        "analyze_clusters_and_stellar_properties(Mass_graphs, Mass_thresholds, data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uv0bEBZlP6jb"
      },
      "source": [
        "#### Plot of degree of distribution, centrality and louvain clustering for Mass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_7GxZPE6cWh"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Distribution\n",
        "for idx, graph in enumerate(Mass_graphs):\n",
        "    title = f\"Graph with Mass Threshold {round(Mass_thresholds[idx],2)}\"\n",
        "\n",
        "    # Degree Distribution\n",
        "    plot_degree_distribution(graph, f\"Degree Distribution: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96xJzPm36cWk"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Degree of Centrality\n",
        "for idx, graph in enumerate(Mass_graphs):\n",
        "    title = f\"Graph with Mass Threshold {round(Mass_thresholds[idx],2)}\"\n",
        "\n",
        "    # Degree Centrality\n",
        "    plot_degree_centrality(graph, f\"Degree Centrality: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OePOCQBh6cWm"
      },
      "outputs": [],
      "source": [
        "# Applying the analysis functions to all four graphs: Louvain Clustering\n",
        "for idx, graph in enumerate(Mass_graphs):\n",
        "    title = f\"Graph with Mass Threshold {round(Mass_thresholds[idx],2)}\"\n",
        "\n",
        "    # Louvain Clustering\n",
        "    plot_louvain_clusters(graph, f\"Louvain Clusters: {title}\")\n",
        "    print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gZ1DuxJ7uiA"
      },
      "source": [
        "# Introducing closeness and centrality for the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX4gCr5S7-EO"
      },
      "source": [
        "***This part has not been implemented, but can be used for further developments of the analysis.***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMPscmG2YlBB"
      },
      "source": [
        "This code calculates closeness centrality and degree centrality for each node (star) in your graph and adds them as new columns to your DataFrame, `data`.\n",
        "\n",
        "- **Closeness centrality** measures the average distance of a node to all other nodes in the graph. Nodes with high closeness centrality are considered more \"central\" because they can reach other nodes quickly.\n",
        "\n",
        "- **Degree centrality** measures the number of connections a node has. Nodes with high degree centrality are considered more \"influential\" because they have direct connections with many other nodes.\n",
        "\n",
        "By adding these metrics as new columns to the DataFrame, you can use them as new features to train machine learning models and achieve more accurate predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUWkNnGeYYrC"
      },
      "outputs": [],
      "source": [
        "# Calcola closeness centrality\n",
        "closeness_centrality = nx.closeness_centrality(G)\n",
        "\n",
        "# Calcola degree centrality\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "\n",
        "# Aggiungi le metriche come nuove colonne al dataframe\n",
        "data['closeness_centrality'] = data['Unnamed: 0'].map(closeness_centrality)\n",
        "data['degree_centrality'] = data['Unnamed: 0'].map(degree_centrality)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a683520-505e-44bf-89c3-03fd84d6666c"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23edfc32-3875-4cb4-b1c0-340dd0f9bae8"
      },
      "source": [
        "## Data splitting\n",
        "\n",
        "Splitting a dataset into 20-80% helps evaluate model performance on unseen data, prevents overfitting, and aids in hyperparameter tuning and model selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d1d29d2-f2fc-492f-b187-eaeb2d44e8c9"
      },
      "outputs": [],
      "source": [
        "# Split arrays or matrices into random train and test subsets.\n",
        "train,test=train_test_split(data, test_size=0.2, random_state=seed, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e14cc5a-04fe-4c1b-ac4b-b44a1674039e"
      },
      "outputs": [],
      "source": [
        "#plotting the diagrams\n",
        "mu_tr, std_tr = hist_with_gaussian(train, 'Age',bins=30,color_graph='red',Gaussian=False,subject='train set')\n",
        "mu_ts, std_ts = hist_with_gaussian(test, 'Age',bins=30, color_graph='blue',Gaussian=False,subject='test set')\n",
        "\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.legend(loc=\"best\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndoQn9s3ow7M"
      },
      "source": [
        "The two datasets seem to be well balanced. Assuming the two distributions to be normal, they share values very similar both in mean value and standard deviation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3764eabf-7a9d-4ade-90e2-54023ed1a96f"
      },
      "source": [
        "# Simple Perceptron\n",
        "\n",
        "We design and implement a simple perceptron model to predict the Age of stars."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9XZoea3YDVG"
      },
      "outputs": [],
      "source": [
        "# Additional library\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmsyMVheUJcD"
      },
      "outputs": [],
      "source": [
        "#### Definition of the functions ####\n",
        "# Function to prepare the data for the model\n",
        "def preparing_data(train,test,scaled):\n",
        "  # Select the only meaningful parameters that needs to be used\n",
        "  if scaled==True:\n",
        "    # function already implemented in the notebook: Only the relevant variables are taken into account\n",
        "    X_train_scaled, X_test_scaled = scaled_train_test(train, test)\n",
        "    X_train = X_train_scaled[ [ 'logg', '[Fe/H]','Teff', 'Rad-Flame','Lum' ,'Mass'] ].values\n",
        "    y_train = X_train_scaled['Age'].values\n",
        "    X_test = X_test_scaled[ [ 'logg', '[Fe/H]', 'Teff','Rad-Flame','Lum', 'Mass'] ].values\n",
        "    y_test = X_test_scaled['Age'].values\n",
        "\n",
        "  else:\n",
        "    X_train = train[[ 'logg', '[Fe/H]', 'Teff','Rad-Flame','Lum','Mass']].values\n",
        "    y_train = train['Age'].values\n",
        "    X_test = test[[ 'logg', '[Fe/H]','Teff', 'Rad-Flame','Lum','Mass']].values\n",
        "    y_test = test['Age'].values\n",
        "\n",
        "  return X_train.shape[1], X_train, y_train, X_test, y_test\n",
        "\n",
        "# Function to creare a simple or multiple layer perceptron\n",
        "def create_mlp_model(architecture, input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Input((input_shape,)))\n",
        "\n",
        "    # Add hidden layers with ReLU activation\n",
        "    for neurons in architecture:\n",
        "        model.add(Dense(neurons, activation='relu'))\n",
        "\n",
        "    # Output layer with 1 neuron\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile the model with AdamW optimizer and MSE loss\n",
        "    model.compile(optimizer=optimizers.AdamW(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
        "    return model\n",
        "\n",
        "# Plot MSE vs epochs for both models\n",
        "def plot_model_curve(history, title,object):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(history.history[object],'o-', label=f\"Model {title}\",color='#2A9D8F')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(object)\n",
        "    plt.title(f\"Plot {object} curve for Model {title}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot predicted vs actual values\n",
        "def plot_pred_vs_actual(y_test, y_pred, title):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.scatter(y_test, y_pred, color='#F4A261', label='Scatterplot')\n",
        "    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='#2A9D8F', lw=2, label='Perfect prediction')  # Line of perfect predictions\n",
        "    plt.title(f'Predicted vs Actual for {title}')\n",
        "    plt.xlabel('Actual Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.show()\n",
        "\n",
        "# Predict on the validation set and compute R2 score\n",
        "def evaluate_model(model, X_val, y_val, model_name,scatterplot):\n",
        "    y_pred = model.predict(X_test).flatten()\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    print('\\n')\n",
        "    print(f'{model_name} R2 score: {r2:.4f}')\n",
        "    print('\\n')\n",
        "\n",
        "    # Plot predicted vs actual values\n",
        "    if scatterplot==True:\n",
        "      plot_pred_vs_actual(y_test, y_pred, model_name)\n",
        "\n",
        "    return r2\n",
        "\n",
        "# Function for printing the data in a tabular form\n",
        "def print_results(results):\n",
        "    # Prepare the data for the table\n",
        "    headers = [\"Model\", \"Architecture\", \"Final Loss\", \"Final Validation MSE\", \"R2 Score\"]\n",
        "    table = []\n",
        "    for i, result in enumerate(results):\n",
        "        row = [\n",
        "            f\"Model {i+1}\",                  # Model number\n",
        "            result['architecture'],           # Architecture of the model\n",
        "            result['final_loss'],             # Final Loss\n",
        "            result['final_val_mse'],          # Final Validation MSE\n",
        "            result['r2_score']                # R2 Score\n",
        "        ]\n",
        "        table.append(row)\n",
        "\n",
        "    # Print the results using tabulate\n",
        "    print(tabulate(table, headers=headers, floatfmt=\".4f\", tablefmt=\"fancy_grid\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "343d0d7f-9c4e-4bef-8ae9-5a69ca734c56"
      },
      "source": [
        "## Initial Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02077528-c15c-4bfb-9958-de3617661a7b"
      },
      "outputs": [],
      "source": [
        "# Function for preparing the data and getting the needed quantities\n",
        "input_data_shape,X_train,y_train,X_test,y_test= preparing_data(train,test,scaled=False)\n",
        "#print(input_data_shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89KSntpbftg5"
      },
      "outputs": [],
      "source": [
        "# Architecture and results' list\n",
        "architecture=[32]\n",
        "results=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a355e374-9820-4554-bc5d-299519ef0c95"
      },
      "source": [
        "## Compile and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd4ee13a-fee2-4277-a7e5-5e806c2ed97b"
      },
      "outputs": [],
      "source": [
        "#### Implementation and study of the model ####\n",
        "print(f\"Training model with architecture: {architecture}\")\n",
        "\n",
        "# Create model\n",
        "model= create_mlp_model(architecture, input_shape=X_train.shape[1])\n",
        "\n",
        "#Prints a string summary of the network.\n",
        "model.summary()\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)\n",
        "\n",
        "# Get final loss and validation MSE\n",
        "final_loss = history.history['loss'][-1]\n",
        "final_val_mse = history.history['val_mse'][-1]\n",
        "\n",
        "# Calculate R2 score\n",
        "r2 = evaluate_model(model, X_test, y_test, f\"Model {architecture}\",scatterplot=True)\n",
        "\n",
        "# Store results\n",
        "results.append({\n",
        "    'architecture': architecture,\n",
        "    'final_loss': final_loss,\n",
        "    'final_val_mse': final_val_mse,\n",
        "    'r2_score': r2\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSkhx6Wff7w3"
      },
      "outputs": [],
      "source": [
        "# Show the results\n",
        "print_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90102d97-ebf1-4a21-8c24-885afd6ca029"
      },
      "source": [
        "## Model Performance\n",
        "\n",
        "Studying the model performance of a neural network is useful to ensure it generalizes well to unseen data, effectively solving the intended problem without overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cca3b807-22b4-40a1-959c-6612c678e9c7"
      },
      "source": [
        "### Loss and MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bc902b4-4f7a-4835-8fd6-1a037b094fa2"
      },
      "outputs": [],
      "source": [
        "# Plot the loss curve\n",
        "plot_model_curve(history,'Simple Perception',object='loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef34c8b5-f7e8-4c04-bb5d-20b04b5f78d6"
      },
      "outputs": [],
      "source": [
        "# Plot the value of loss curve\n",
        "plot_model_curve(history,'Simple Perception',object='val_loss')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyWdHI-ZhtQM"
      },
      "outputs": [],
      "source": [
        "# Plot the MSE curve\n",
        "plot_model_curve(history,'Simple Perception',object='mse')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6lO9BVThaxp"
      },
      "outputs": [],
      "source": [
        "# Plot the value of MSE curve\n",
        "plot_model_curve(history,'Simple Perception',object='val_mse')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81b817b3-f7b7-4196-8973-d7c04d191d22"
      },
      "source": [
        "# Multilayer Perceptron\n",
        "\n",
        "A multilayer perceptron is beneficial because it can capture complex patterns and relationships in data through its multiple layers and non-linear activation functions, making it more powerful than single-layer networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd2ec5a4-a67c-4017-bd43-55c802190f9d"
      },
      "source": [
        "## Initial MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9N8lhezgEc-m"
      },
      "outputs": [],
      "source": [
        "#### Definition of the functions ####\n",
        "# Function for finding the best architecture\n",
        "def find_best_architecture(results):\n",
        "    best_architecture = None\n",
        "    best_r2 = -float('inf')\n",
        "    for result in results:\n",
        "        r2 = result['r2_score']\n",
        "        architecture = result['architecture']\n",
        "\n",
        "        if r2 > best_r2:\n",
        "            best_r2 = r2\n",
        "            best_architecture = architecture\n",
        "\n",
        "    # Print the architecure\n",
        "    print(f\"Best R2 score: {best_r2:.4f}\")\n",
        "    print(f\"Best architecture: {best_architecture}\")\n",
        "\n",
        "    return best_r2, best_architecture\n",
        "\n",
        "# Function for finding the worst architecture\n",
        "def find_worst_architecture(results):\n",
        "    worst_architecture = None\n",
        "    worst_r2 = float('inf')\n",
        "    for result in results:\n",
        "        r2 = result['r2_score']\n",
        "        architecture = result['architecture']\n",
        "\n",
        "        if r2 < worst_r2 and r2>0:\n",
        "            worst_r2 = r2\n",
        "            worst_architecture = architecture\n",
        "\n",
        "    # Print the architecure\n",
        "    print(f\"Worst R2 score: {worst_r2:.4f}\")\n",
        "    print(f\"Worst architecture: {worst_architecture}\")\n",
        "\n",
        "    return worst_r2, worst_architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MU2A59KFirqQ"
      },
      "outputs": [],
      "source": [
        "#### Model architectures ####\n",
        "architectures = [\n",
        "    [128, 256, 512],\n",
        "    [512, 256, 128],\n",
        "    [256, 512, 512, 256],\n",
        "    [768, 128]\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNClW58yE7eJ"
      },
      "outputs": [],
      "source": [
        "#### Train and evaluate multiple models ####\n",
        "# Prepare to store results\n",
        "results = []\n",
        "for architecture in architectures:\n",
        "    print(f\"Training model with architecture: {architecture}\")\n",
        "\n",
        "    # Create model\n",
        "    model= create_mlp_model(architecture, input_shape=X_train.shape[1])\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)\n",
        "\n",
        "    # Get final loss and validation MSE\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    final_val_mse = history.history['val_mse'][-1]\n",
        "\n",
        "    # Calculate R2 score\n",
        "    r2 = evaluate_model(model, X_test, y_test, f\"Model {architecture}\",scatterplot=True)\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        'architecture': architecture,\n",
        "        'final_loss': final_loss,\n",
        "        'final_val_mse': final_val_mse,\n",
        "        'r2_score': r2\n",
        "    })\n",
        "    # Plot of the relevant curves\n",
        "    plot_model_curve(history, architecture,object='loss')\n",
        "    plot_model_curve(history, architecture,object='val_loss')\n",
        "    print('########################################################################')\n",
        "    print('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE8j5dMsE_Cx"
      },
      "outputs": [],
      "source": [
        "# Print out results\n",
        "print_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1viJa-z2dj9S"
      },
      "outputs": [],
      "source": [
        "# Show the best architecture\n",
        "Best_R2, Best_architecture=find_best_architecture(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyYuakPbYapP"
      },
      "outputs": [],
      "source": [
        "# Save the results\n",
        "results_normal=results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85bd339c-5ace-4d66-92ce-02d942b03e0c"
      },
      "source": [
        "# Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42e525ac-e21d-4d64-9952-fff6910d8df2"
      },
      "source": [
        "## Data scaling\n",
        "\n",
        "When working with machine learning models, especially neural networks, having features\n",
        "on different scales can significantly impact the model’s performance. Here’s why:\n",
        "- **Convergence issues:** Features with larger scales can dominate the learning\n",
        "process, causing the model to converge slowly or get stuck in suboptimal\n",
        "solutions.\n",
        "- **Biased importance:** The model might incorrectly assign more importance to\n",
        "features with larger magnitudes, even if they’re not actually more predictive.\n",
        "- **Gradient descent problems:** During optimization, features with larger scales cancause larger gradients, leading to unstable updates and potentially overshooting optimal values.\n",
        "- **Activation function saturation:** For neural networks, large input values can push activation functions like ReLU or sigmoid into saturation regions, slowing down learning.\n",
        "\n",
        "These issues can result in poor model performance, reflected in low $𝑅^2$ scores. By scaling your data, you can ensure that all features contribute more equally to the learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pq6F3HD_mhMv"
      },
      "outputs": [],
      "source": [
        "#### Definition of the function ####\n",
        "\n",
        "# Function to scale the data\n",
        "def scaled_train_test(df_train: pd.DataFrame, df_test: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    assert not df_train.empty, \"Training DataFrame is empty\"\n",
        "    assert not df_test.empty, \"Test DataFrame is empty\"\n",
        "\n",
        "    if not df_train.columns.equals(df_test.columns):\n",
        "        raise ValueError(\"Training and test DataFrames must have the same columns\")\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaled_df_train = pd.DataFrame(scaler.fit_transform(df_train), columns=df_train.columns)\n",
        "    scaled_df_test = pd.DataFrame(scaler.transform(df_test), columns=df_test.columns)\n",
        "\n",
        "    return scaled_df_train, scaled_df_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FC9frrzEIvM"
      },
      "outputs": [],
      "source": [
        "# Function for preparing the data and getting the needed quantities\n",
        "input_data_shape,X_train,y_train, X_test,y_test= preparing_data(train,test,scaled=True) # Scaled=True, hence the data will be scaled\n",
        "#print(input_data_shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-j0cxIaJulD"
      },
      "outputs": [],
      "source": [
        "# Train and evaluate multiple models\n",
        "results = []\n",
        "for architecture in architectures:\n",
        "    print(f\"Training model with architecture: {architecture}, scaled case\")\n",
        "\n",
        "    # Create model\n",
        "    model= create_mlp_model(architecture, input_shape=input_data_shape)\n",
        "\n",
        "    # Train model\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)\n",
        "\n",
        "    # Get final loss and validation MSE\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    final_val_mse = history.history['val_mse'][-1]\n",
        "\n",
        "    # Calculate R2 score\n",
        "    r2 = evaluate_model(model, X_test, y_test, f\"Model {architecture}\",scatterplot=True)\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        'architecture': architecture,\n",
        "        'final_loss': final_loss,\n",
        "        'final_val_mse': final_val_mse,\n",
        "        'r2_score': r2\n",
        "    })\n",
        "    # Plot of the relevant curves\n",
        "    plot_model_curve(history, architecture,object='loss')\n",
        "    plot_model_curve(history, architecture,object='val_loss')\n",
        "    print('########################################################################')\n",
        "    print('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9h3ynjrkziF"
      },
      "outputs": [],
      "source": [
        "# Print out results\n",
        "print_results(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erzC8Du7bmi1"
      },
      "outputs": [],
      "source": [
        "# Save the data\n",
        "results_scaled=results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB6eo0_HxyNc"
      },
      "outputs": [],
      "source": [
        "# Compare the results wihthin the architectures\n",
        "Best_R2_scaled, Best_architecture_scaled=find_best_architecture(results_scaled)\n",
        "Worst_R2_scaled, Worst_architecture_scaled=find_worst_architecture(results_scaled)\n",
        "print('\\n')\n",
        "Best_R2_normal, Best_architecture_normal=find_best_architecture(results_normal)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCkwRBb4b97S"
      },
      "source": [
        "We can clearly notice the difference between before and after the scalar procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "465f5085-da82-4c71-82c1-9a2f4cc624bc"
      },
      "source": [
        "## Helpfull layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38da779b-e31c-4e53-a89f-1091c5b1c6b1"
      },
      "outputs": [],
      "source": [
        "#### Definition of the functions ####\n",
        "# Function to create a model with different configurations\n",
        "def create_model_with_layers(case, architecture, input_shape, dropout_rate=0.3):\n",
        "    model = Sequential()\n",
        "\n",
        "    if case == 'B':\n",
        "        # FIX: Pass input_shape as a tuple\n",
        "        model.add(BatchNormalization(input_shape=(input_shape,)))\n",
        "        for neurons in architecture:\n",
        "            model.add(Dense(neurons, activation='relu'))\n",
        "    else:\n",
        "        model.add(Input((input_shape,)))\n",
        "        model.add(Dense(architecture[0], activation='relu'))\n",
        "\n",
        "        if case == 'A':\n",
        "            model.add(Dropout(dropout_rate))\n",
        "        elif case == 'C':\n",
        "            model.add(BatchNormalization())\n",
        "        elif case == 'D':\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Add the rest of the hidden layers for all cases except E\n",
        "    if case != 'E':\n",
        "        for neurons in architecture[1:]:\n",
        "            model.add(Dense(neurons, activation='relu'))\n",
        "    else:\n",
        "        # In case E, apply batch normalization only after hidden layers\n",
        "        for neurons in architecture:\n",
        "            model.add(Dense(neurons, activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile model with AdamW optimizer and MSE loss\n",
        "    model.compile(optimizer=optimizers.AdamW(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to print the results for multiple cases\n",
        "def print_results_multicase(results):\n",
        "    # Prepare the data for the table\n",
        "    headers = [\"Model\", \"Architecture\", \"Final Loss\", \"Final Validation MSE\", \"R2 Score\"]\n",
        "\n",
        "    # Create separate tables for each case\n",
        "    case_tables = {case: [] for case in ['A', 'B', 'C', 'D', 'E']}\n",
        "\n",
        "    for i, result in enumerate(results):\n",
        "        row = [\n",
        "            f\"Model {i+1}\",\n",
        "            result['architecture'],\n",
        "            result['final_loss'],\n",
        "            result['final_val_mse'],\n",
        "            result['r2_score']\n",
        "        ]\n",
        "        # Assign the row to the correct case table\n",
        "        case = result.get('case')  # Assumes 'case' is added to the results dict\n",
        "        if case in case_tables:\n",
        "            case_tables[case].append(row)\n",
        "        else:\n",
        "            print(f\"Warning: Unknown case '{case}' found in results. Ignoring.\")\n",
        "\n",
        "    # Print tables for each case\n",
        "    for case, table in case_tables.items():\n",
        "        if table:\n",
        "            print(f\"## Results for Case {case}:\")\n",
        "            print(tabulate(table, headers=headers, floatfmt=\".4f\", tablefmt=\"latex\"))\n",
        "            print(\"\\n\")\n",
        "        else:\n",
        "            print(f\"No results found for Case {case}.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfAHE7b1kW4a"
      },
      "outputs": [],
      "source": [
        "# Experiment with different models and compare their performance\n",
        "cases = ['A', 'B', 'C', 'D', 'E']\n",
        "dropout_rates = [0.2, 0.3, 0.4, 0.5]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKDGsaSmcwjj"
      },
      "outputs": [],
      "source": [
        "# Variables unscaled\n",
        "#input_data_shape,X_train,y_train, X_test,y_test= preparing_data(train,test,scaled=False) # Scaled=False, hence the data will not be scaled\n",
        "#print(input_data_shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny-hmRdx-stE"
      },
      "source": [
        "I decided to use scalar data because the model associated have an $R^2$ which is close to 1, hence more affected by the overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-RnrC3qfWKM"
      },
      "outputs": [],
      "source": [
        "# Variables scaled\n",
        "input_data_shape,X_train,y_train, X_test,y_test= preparing_data(train,test,scaled=True) # Scaled=True, hence the data will be scaled\n",
        "#print(input_data_shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm-Jffl2kcSs"
      },
      "outputs": [],
      "source": [
        "#### IMplementation of the functions ####\n",
        "\n",
        "best_r2, best_architecture= find_best_architecture(results_scaled)\n",
        "results_multicase = []\n",
        "\n",
        "# Train models for each case and plot loss curves\n",
        "for case in cases:\n",
        "    for dropout_rate in dropout_rates if case == 'A' or case == 'D' else [None]:\n",
        "        print('\\n')\n",
        "        print('######## Case',case,'########')\n",
        "        print(f\"Training case {case} with dropout rate {dropout_rate}\" if dropout_rate else f\"Training case {case}\")\n",
        "        print('\\n')\n",
        "\n",
        "        # Create model for the case\n",
        "        model = create_model_with_layers(case, best_architecture, input_shape=input_data_shape, dropout_rate=dropout_rate)\n",
        "\n",
        "        # Train model for 10 epochs\n",
        "        history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=0)\n",
        "\n",
        "        # Get final loss and validation MSE\n",
        "        final_loss = history.history['loss'][-1]\n",
        "        final_val_mse = history.history['val_mse'][-1]\n",
        "\n",
        "        # Calculate R2 score\n",
        "        r2 = evaluate_model(model, X_test, y_test, f\"Model {best_architecture}\",scatterplot=True)\n",
        "\n",
        "        # Store results\n",
        "        results_multicase.append({\n",
        "            'case': case,  # Add the case to the dictionary\n",
        "            'architecture': best_architecture,\n",
        "            'final_loss': final_loss,\n",
        "            'final_val_mse': final_val_mse,\n",
        "            'r2_score': r2\n",
        "        })\n",
        "\n",
        "        # Plot of the relevant curves\n",
        "        plot_model_curve(history, best_architecture,object='loss')\n",
        "        plot_model_curve(history, best_architecture,object='val_loss')\n",
        "        print('########################################################################')\n",
        "        print('\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4y57uWZa63fw"
      },
      "outputs": [],
      "source": [
        "# Print the results\n",
        "print_results_multicase(results_multicase)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f730a7d0-7642-497b-9796-95bfbc3867d1"
      },
      "source": [
        "## Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vf_TrbzGTEGw"
      },
      "outputs": [],
      "source": [
        "# Additional library for regularizers\n",
        "from keras import regularizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0mw0g37LRa2"
      },
      "outputs": [],
      "source": [
        "#### Definition of the functions ####\n",
        "# Function to create a model with L1, L2 or L1_L2 regularization\n",
        "def create_model_with_regularization(regularizer_type, architecture, input_shape, alpha):\n",
        "    model = Sequential()\n",
        "\n",
        "    regularizer = None\n",
        "    if regularizer_type == 'L1':\n",
        "        regularizer = regularizers.l1(alpha)\n",
        "    elif regularizer_type == 'L2':\n",
        "        regularizer = regularizers.l2(alpha)\n",
        "    elif regularizer_type == 'L1_L2':\n",
        "        regularizer = regularizers.l1_l2(l1=alpha, l2=alpha)\n",
        "\n",
        "    # Input layer and first Dense layer with regularization\n",
        "    model.add(Input((input_shape,)))\n",
        "    model.add(Dense(architecture[0], activation='relu', kernel_regularizer=regularizer))\n",
        "\n",
        "    # Add other hidden layers with regularization\n",
        "    for neurons in architecture[1:]:\n",
        "        model.add(Dense(neurons, activation='relu', kernel_regularizer=regularizer))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # Compile model with AdamW optimizer and MSE loss\n",
        "    model.compile(optimizer=optimizers.AdamW(learning_rate=0.001), loss='mse', metrics=['mse'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Function to show the results\n",
        "def print_reg_results(results):\n",
        "    # Prepare the data for the table\n",
        "    headers = [\"Regularizer\", \"Alpha\", \"Architecture\", \"Final Loss\", \"Final Validation MSE\", \"R2 Score\"]\n",
        "    table = []\n",
        "    for i, result in enumerate(results):\n",
        "        row = [\n",
        "            result['regularizer_type'],  # Regularizer type\n",
        "            result['alpha'],            # Alpha value\n",
        "            result['architecture'],       # Architecture of the model\n",
        "            result['final_loss'],         # Final Loss\n",
        "            result['final_val_mse'],      # Final Validation MSE\n",
        "            result['r2_score']            # R2 Score\n",
        "        ]\n",
        "        table.append(row)\n",
        "\n",
        "    # Print the results using tabulate\n",
        "    print(tabulate(table, headers=headers, floatfmt=\".4f\", tablefmt=\"latex\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CJ72MCm3gwgo",
        "outputId": "a5dbd816-f0d0-4a0a-bf5b-8a1d514a12e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best R2 score: 0.8317\n",
            "Best architecture: [256, 512, 512, 256]\n"
          ]
        }
      ],
      "source": [
        "# Best architecture found previously\n",
        "best_r2,best_architecture= find_best_architecture(results_scaled)\n",
        "\n",
        "# Regularization parameter alpha\n",
        "alpha = 0.001\n",
        "\n",
        "# Experiment with L1, L2, and L1_L2 regularization and compare the results\n",
        "regularization_types = ['L1', 'L2', 'L1_L2']\n",
        "results_reg = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45ea4d5b-1422-4175-8aa6-24de1ff5edc4"
      },
      "outputs": [],
      "source": [
        "# Train models with different regularization types\n",
        "for regularizer_type in regularization_types:\n",
        "    print(f\"Training model with {regularizer_type} regularization (alpha={alpha})\")\n",
        "\n",
        "    # Create model with specified regularization\n",
        "    model = create_model_with_regularization(regularizer_type, best_architecture, input_data_shape, alpha=alpha)\n",
        "\n",
        "    # Train model for 10 epochs\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, verbose=2)\n",
        "\n",
        "    # Get final loss and validation MSE\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    final_val_mse = history.history['val_mse'][-1]\n",
        "\n",
        "    # Calculate R2 score\n",
        "    r2 = evaluate_model(model, X_test, y_test, f\"Model {best_architecture}\",scatterplot=True)\n",
        "\n",
        "    # Store results\n",
        "    results_reg.append({\n",
        "        'regularizer_type': regularizer_type,\n",
        "        'alpha': alpha,\n",
        "        'architecture': best_architecture,\n",
        "        'final_loss': final_loss,\n",
        "        'final_val_mse': final_val_mse,\n",
        "        'r2_score': r2\n",
        "    })\n",
        "\n",
        "    # Plot of the relevant curves\n",
        "    plot_model_curve(history,  best_architecture,object='loss')\n",
        "    plot_model_curve(history,  best_architecture,object='val_loss')\n",
        "    print('########################################################################')\n",
        "    print('\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5824370-15e9-4aff-ae0c-b1feb644be16"
      },
      "outputs": [],
      "source": [
        "# Print results\n",
        "print_reg_results(results_reg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3b40e67-0689-4182-a3be-b086985143aa"
      },
      "source": [
        "### Epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFznIxLKM9pU"
      },
      "outputs": [],
      "source": [
        "#### Definition of the functions ####\n",
        "\n",
        "# Plot MSE vs epochs for both models\n",
        "def plot_mse_vs_epochs(history, model_name):\n",
        "    plt.plot(history.history['mse'], label='Training MSE')\n",
        "    plt.plot(history.history['val_mse'], label='Validation MSE')\n",
        "    plt.title(f'MSE vs Epochs for {model_name}')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Mean Squared Error (MSE)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Function to find the second best architecture\n",
        "def find_second_best_architecture(results):\n",
        "    # Sort results by R2 score in descending order\n",
        "    sorted_results = sorted(results, key=lambda x: x['r2_score'], reverse=True)\n",
        "\n",
        "    # Return the second best R2 score and architecture\n",
        "    if len(sorted_results) > 1:  # Check if there's at least a second best model\n",
        "        second_best_r2 = sorted_results[1]['r2_score']\n",
        "        second_best_architecture = sorted_results[1]['architecture']\n",
        "        print(f\"Second best R2 score: {second_best_r2:.4f}\")\n",
        "        print(f\"Second best architecture: {second_best_architecture}\")\n",
        "        return second_best_r2, second_best_architecture\n",
        "    else:\n",
        "        print(\"There is only one model in the results, cannot determine the second best.\")\n",
        "        return None, None  # or raise an exception if appropriate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZkeUZjodm6bi"
      },
      "outputs": [],
      "source": [
        "# First and second best model\n",
        "best_r2_1, best_model_1_architecture = find_best_architecture(results_scaled)\n",
        "best_r2_2, best_model_2_architecture = find_second_best_architecture(results_scaled)\n",
        "\n",
        "\n",
        "# List of the two best-performing architectures\n",
        "architectures = [best_model_1_architecture, best_model_2_architecture]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b779f75-1553-42ca-8ac6-6141784bfdf1"
      },
      "outputs": [],
      "source": [
        "#### Implementation of the function ####\n",
        "\n",
        "results_100 = []\n",
        "# Iterate through each best architecture and train a model\n",
        "for architecture in architectures:\n",
        "    print(f\"Training model with architecture: {architecture}, scaled case\")\n",
        "\n",
        "    # Create model with the specified architecture\n",
        "    model = create_mlp_model(architecture, input_shape=input_data_shape)\n",
        "\n",
        "    # Train the model for 100 epochs\n",
        "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=2)\n",
        "\n",
        "    # Final training loss and validation MSE\n",
        "    final_loss = history.history['loss'][-1]\n",
        "    final_val_mse = history.history['val_mse'][-1]\n",
        "\n",
        "    # Calculate the R² score on the test set\n",
        "    r2 = evaluate_model(model, X_test, y_test, f\"Model {architecture}\", scatterplot=True)\n",
        "\n",
        "    # Store the results for this architecture\n",
        "    results_100.append({\n",
        "        'architecture': architecture,\n",
        "        'final_loss': final_loss,\n",
        "        'final_val_mse': final_val_mse,\n",
        "        'r2_score': r2\n",
        "    })\n",
        "\n",
        "    # Plot MSE vs Epochs\n",
        "    print('\\n')\n",
        "    print('########################################################################')\n",
        "    plot_mse_vs_epochs(history, f\"{architecture}\")\n",
        "    print('########################################################################')\n",
        "    print('\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzjMwe_inTSK"
      },
      "outputs": [],
      "source": [
        "# Present the final results\n",
        "print_results(results_100)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}